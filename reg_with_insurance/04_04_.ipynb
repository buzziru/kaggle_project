{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 738 ms, total: 1.75 s\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import optuna\n",
    "from xgboost import XGBRegressor, callback\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=2024\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.16 s, sys: 3.17 s, total: 12.3 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def load_data():    \n",
    "    train = pd.read_csv('data/train.csv')\n",
    "    test = pd.read_csv('data/test.csv')    \n",
    "    all_df = pd.concat([train, test], sort=False).reset_index(drop=True)\n",
    "    return train, test, all_df\n",
    "\n",
    "def fill_nan_values(df):\n",
    "    num_cols = [col for col in df.select_dtypes(exclude='object').columns if col != 'Premium Amount']\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna('missing')\n",
    "    return df\n",
    "    \n",
    "def skewed(df, all_df):\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    pt.fit(df[['Annual Income']])\n",
    "    all_df['transformed_Annual_Income'] = pt.transform(all_df[['Annual Income']])\n",
    "    # all_df['log_Annual_Income'] = np.log1p(all_df['Annual Income'])\n",
    "    return all_df\n",
    "    \n",
    "def date(df):\n",
    "    df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'])\n",
    "    df['Year'] = df['Policy Start Date'].dt.year\n",
    "    df['Day'] = df['Policy Start Date'].dt.day\n",
    "    df['Month'] = df['Policy Start Date'].dt.month\n",
    "    df['Month_name'] = df['Policy Start Date'].dt.month_name()\n",
    "    df['Day_of_week'] = df['Policy Start Date'].dt.day_name()\n",
    "    df['Week'] = df['Policy Start Date'].dt.isocalendar().week\n",
    "    df['Year_sin'] = np.sin(2 * np.pi * df['Year'])\n",
    "    df['Year_cos'] = np.cos(2 * np.pi * df['Year'])\n",
    "    min_year = df['Year'].min()\n",
    "    max_year = df['Year'].max()\n",
    "    df['Year_sin'] = np.sin(2 * np.pi * (df['Year'] - min_year) / (max_year - min_year))\n",
    "    df['Year_cos'] = np.cos(2 * np.pi * (df['Year'] - min_year) / (max_year - min_year))\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12) \n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)  \n",
    "    df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n",
    "    df['Group']=(df['Year']-2020)*48+df['Month']*4+df['Day']//7    \n",
    "    df.drop('Policy Start Date', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_nan_cols(df):\n",
    "    nan_cols = ['Marital Status', 'Customer Feedback', 'Health Score', 'Previous Claims', 'Vehicle Age', 'Credit Score', 'Insurance Duration']\n",
    "    for col in nan_cols:\n",
    "        col_name = col + '_NA'\n",
    "        df[col_name] = df[col].isnull().astype(int)\n",
    "    return df\n",
    "\n",
    "def get_encoding(df):\n",
    "    def encode_ordinal(df):\n",
    "        educ = {\"High School\":0, \"Bachelor's\":1, \"Master's\":2, \"PhD\":3}\n",
    "        policy = {'Basic':0, 'Comprehensive':1, 'Premium':2}\n",
    "        exerc = {'Rarely':0, 'Daily':1, 'Weekly':2, 'Monthly': 3}\n",
    "        # feedback = {'Poor':0, 'Average':1, 'Good':2}\n",
    "\n",
    "        df['Education Level'] = df['Education Level'].map(educ)\n",
    "        df['Policy Type'] = df['Policy Type'].map(policy)\n",
    "        df['Exercise Frequency'] = df['Exercise Frequency'].map(exerc)\n",
    "        # df['Customer Feedback'] = df['Customer Feedback'].map(feedback)\n",
    "        df['Gender'] = df['Gender'].map({'Male':0, 'Female':1})\n",
    "        df['Smoking Status'] = df['Smoking Status'].map({'Yes':1, 'No':0})\n",
    "        return df\n",
    "    \n",
    "    def target_encoder(df):\n",
    "        train = df[~df['Premium Amount'].isnull()]\n",
    "        test = df[df['Premium Amount'].isnull()]\n",
    "        encoder = TargetEncoder()\n",
    "        categorical_cols = ['Marital Status', 'Customer Feedback']\n",
    "        train[categorical_cols] = encoder.fit_transform(train[categorical_cols], train['Premium Amount'])\n",
    "        test[categorical_cols] = encoder.transform(test[categorical_cols])\n",
    "        df = pd.concat([train, test], sort=False).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def one_hot_dummies(df, categorical):\n",
    "        oh = pd.get_dummies(df[categorical])\n",
    "        df = df.drop(categorical, axis=1)\n",
    "        return pd.concat([df, oh], axis=1)\n",
    "        return df\n",
    "\n",
    "    df = encode_ordinal(df)\n",
    "    df = target_encoder(df)\n",
    "\n",
    "    categorical_features = df.select_dtypes(include='object').columns\n",
    "    df = one_hot_dummies(df, categorical_features)\n",
    "    return df\n",
    "\n",
    "def add_new_features(df):\n",
    "    df['Income_Dependents Ratio'] = df['Annual Income'] / (df['Number of Dependents'].fillna(0) + 1)\n",
    "    df['Income_per_Dependent'] = df['Annual Income'] / (df['Number of Dependents'] + 1)\n",
    "    df['CreditScore_InsuranceDuration'] = df['Credit Score'] * df['Insurance Duration']\n",
    "    df['Health_Risk_Score'] = df['Smoking Status'].apply(lambda x: 1 if x == 'Smoker' else 0) + \\\n",
    "                                df['Exercise Frequency'].apply(lambda x: 1 if x == 'Low' else (0.5 if x == 'Medium' else 0)) + \\\n",
    "                                (100 - df['Health Score']) / 20\n",
    "    df['Credit_Health_Score'] = df['Credit Score'] * df['Health Score']\n",
    "    df['Health_Age_Interaction'] = df['Health Score'] * df['Age']\n",
    "\n",
    "    df['contract_length'] = pd.cut(\n",
    "        df[\"Insurance Duration\"].fillna(99),  \n",
    "        bins=[-float('inf'), 1, 3, float('inf')],  \n",
    "        labels=[0, 1, 2]  \n",
    "    ).astype(int)\n",
    "\n",
    "    df['Age_Income'] = df['Age'] * df['Annual Income']\n",
    "\n",
    "    # df[\"Annual_Income_Health_Score_Ratio\"] = df[\"Health Score\"] / df[\"Annual Income\"]\n",
    "    # df[\"Annual_Income_Age_Ratio\"] = df[\"Annual Income\"] / df[\"Age\"]\n",
    "    # df[\"Credit_Age\"] = df[\"Credit Score\"] / df[\"Age\"]\n",
    "    # df[\"Vehicle_Age_Insurance_Duration\"] = df[\"Vehicle Age\"] / df[\"Insurance Duration\"]\n",
    "    return df\n",
    "\n",
    "def prep():\n",
    "    train, test, all_df = load_data()\n",
    "\n",
    "    all_df = skewed(train, all_df)\n",
    "    all_df = date(all_df)\n",
    "    all_df = get_nan_cols(all_df)\n",
    "    # all_df = fill_nan_values(all_df)\n",
    "    all_df = get_encoding(all_df)\n",
    "    all_df = add_new_features(all_df)\n",
    "\n",
    "    del all_df['Annual Income']\n",
    "    \n",
    "    train = all_df[~all_df['Premium Amount'].isnull()]\n",
    "    test = all_df[all_df['Premium Amount'].isnull()]\n",
    "    train.drop('id', axis=1, inplace=True)\n",
    "    test.drop(['id', 'Premium Amount'], axis=1, inplace=True)\n",
    "    return train, test, all_df\n",
    "\n",
    "def prep_nan():\n",
    "    train, test, all_df = load_data()\n",
    "\n",
    "    all_df = skewed(train, all_df)\n",
    "    all_df = date(all_df)\n",
    "    all_df = get_nan_cols(all_df)\n",
    "    all_df = fill_nan_values(all_df)\n",
    "    all_df = get_encoding(all_df)\n",
    "    all_df = add_new_features(all_df)\n",
    "\n",
    "    del all_df['Annual Income']\n",
    "    \n",
    "    train = all_df[~all_df['Premium Amount'].isnull()]\n",
    "    test = all_df[all_df['Premium Amount'].isnull()]\n",
    "    train.drop('id', axis=1, inplace=True)\n",
    "    test.drop(['id', 'Premium Amount'], axis=1, inplace=True)\n",
    "    return train, test, all_df\n",
    "\n",
    "train, test, all_df = prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Health Score</th>\n",
       "      <th>Policy Type</th>\n",
       "      <th>Previous Claims</th>\n",
       "      <th>Vehicle Age</th>\n",
       "      <th>Credit Score</th>\n",
       "      <th>Insurance Duration</th>\n",
       "      <th>Customer Feedback</th>\n",
       "      <th>Smoking Status</th>\n",
       "      <th>Exercise Frequency</th>\n",
       "      <th>Premium Amount</th>\n",
       "      <th>transformed_Annual_Income</th>\n",
       "      <th>Year</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Year_sin</th>\n",
       "      <th>Year_cos</th>\n",
       "      <th>Month_sin</th>\n",
       "      <th>Month_cos</th>\n",
       "      <th>Day_sin</th>\n",
       "      <th>Day_cos</th>\n",
       "      <th>Group</th>\n",
       "      <th>Marital Status_NA</th>\n",
       "      <th>Customer Feedback_NA</th>\n",
       "      <th>Health Score_NA</th>\n",
       "      <th>Previous Claims_NA</th>\n",
       "      <th>Vehicle Age_NA</th>\n",
       "      <th>Credit Score_NA</th>\n",
       "      <th>Insurance Duration_NA</th>\n",
       "      <th>Occupation_Employed</th>\n",
       "      <th>Occupation_Self-Employed</th>\n",
       "      <th>Occupation_Unemployed</th>\n",
       "      <th>Location_Rural</th>\n",
       "      <th>Location_Suburban</th>\n",
       "      <th>Location_Urban</th>\n",
       "      <th>Property Type_Apartment</th>\n",
       "      <th>Property Type_Condo</th>\n",
       "      <th>Property Type_House</th>\n",
       "      <th>Month_name_April</th>\n",
       "      <th>Month_name_August</th>\n",
       "      <th>Month_name_December</th>\n",
       "      <th>Month_name_February</th>\n",
       "      <th>Month_name_January</th>\n",
       "      <th>Month_name_July</th>\n",
       "      <th>Month_name_June</th>\n",
       "      <th>Month_name_March</th>\n",
       "      <th>Month_name_May</th>\n",
       "      <th>Month_name_November</th>\n",
       "      <th>Month_name_October</th>\n",
       "      <th>Month_name_September</th>\n",
       "      <th>Day_of_week_Friday</th>\n",
       "      <th>Day_of_week_Monday</th>\n",
       "      <th>Day_of_week_Saturday</th>\n",
       "      <th>Day_of_week_Sunday</th>\n",
       "      <th>Day_of_week_Thursday</th>\n",
       "      <th>Day_of_week_Tuesday</th>\n",
       "      <th>Day_of_week_Wednesday</th>\n",
       "      <th>Income_Dependents Ratio</th>\n",
       "      <th>Income_per_Dependent</th>\n",
       "      <th>CreditScore_InsuranceDuration</th>\n",
       "      <th>Health_Risk_Score</th>\n",
       "      <th>Credit_Health_Score</th>\n",
       "      <th>Health_Age_Interaction</th>\n",
       "      <th>contract_length</th>\n",
       "      <th>Age_Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1099.844389</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.598761</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1098.892745</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2869.0</td>\n",
       "      <td>-0.596487</td>\n",
       "      <td>2023</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>-9.510565e-01</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5024.5</td>\n",
       "      <td>5024.5</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>3.870062</td>\n",
       "      <td>8406.738970</td>\n",
       "      <td>429.376453</td>\n",
       "      <td>2</td>\n",
       "      <td>190931.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1100.625116</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>15.569731</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1094.350977</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1483.0</td>\n",
       "      <td>0.336563</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>-9.510565e-01</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.651372</td>\n",
       "      <td>-0.758758</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7919.5</td>\n",
       "      <td>7919.5</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>4.221513</td>\n",
       "      <td>10805.393307</td>\n",
       "      <td>607.219509</td>\n",
       "      <td>1</td>\n",
       "      <td>1235442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1100.625116</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.177549</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1096.284299</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.140781</td>\n",
       "      <td>2023</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>-9.510565e-01</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.201299</td>\n",
       "      <td>0.979530</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6400.5</td>\n",
       "      <td>6400.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.641123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1085.083634</td>\n",
       "      <td>1</td>\n",
       "      <td>588846.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1099.844389</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.938144</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1098.892745</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>765.0</td>\n",
       "      <td>2.088459</td>\n",
       "      <td>2024</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.651372</td>\n",
       "      <td>-0.758758</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>47285.0</td>\n",
       "      <td>47285.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>4.453093</td>\n",
       "      <td>4014.298906</td>\n",
       "      <td>229.701027</td>\n",
       "      <td>0</td>\n",
       "      <td>2978955.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1101.735535</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.376094</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1098.892745</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>0.555622</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>5.877853e-01</td>\n",
       "      <td>-0.809017</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.979530</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>19825.5</td>\n",
       "      <td>19825.5</td>\n",
       "      <td>2392.0</td>\n",
       "      <td>3.981195</td>\n",
       "      <td>12184.903989</td>\n",
       "      <td>427.897966</td>\n",
       "      <td>2</td>\n",
       "      <td>832671.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender  Marital Status  Number of Dependents  Education Level  \\\n",
       "0  19.0       1     1099.844389                   1.0                1   \n",
       "1  39.0       1     1100.625116                   3.0                2   \n",
       "2  23.0       0     1100.625116                   3.0                0   \n",
       "3  21.0       0     1099.844389                   2.0                1   \n",
       "4  21.0       0     1101.735535                   1.0                1   \n",
       "\n",
       "   Health Score  Policy Type  Previous Claims  Vehicle Age  Credit Score  \\\n",
       "0     22.598761            2              2.0         17.0         372.0   \n",
       "1     15.569731            1              1.0         12.0         694.0   \n",
       "2     47.177549            2              1.0         14.0           NaN   \n",
       "3     10.938144            0              1.0          0.0         367.0   \n",
       "4     20.376094            2              0.0          8.0         598.0   \n",
       "\n",
       "   Insurance Duration  Customer Feedback  Smoking Status  Exercise Frequency  \\\n",
       "0                 5.0        1098.892745               0                   2   \n",
       "1                 2.0        1094.350977               1                   3   \n",
       "2                 3.0        1096.284299               1                   2   \n",
       "3                 1.0        1098.892745               1                   1   \n",
       "4                 4.0        1098.892745               1                   2   \n",
       "\n",
       "   Premium Amount  transformed_Annual_Income  Year  Day  Month  Week  \\\n",
       "0          2869.0                  -0.596487  2023   23     12    51   \n",
       "1          1483.0                   0.336563  2023   12      6    24   \n",
       "2           567.0                   0.140781  2023   30      9    39   \n",
       "3           765.0                   2.088459  2024   12      6    24   \n",
       "4          2022.0                   0.555622  2021    1     12    48   \n",
       "\n",
       "       Year_sin  Year_cos     Month_sin     Month_cos   Day_sin   Day_cos  \\\n",
       "0 -9.510565e-01  0.309017 -2.449294e-16  1.000000e+00 -0.998717 -0.050649   \n",
       "1 -9.510565e-01  0.309017  1.224647e-16 -1.000000e+00  0.651372 -0.758758   \n",
       "2 -9.510565e-01  0.309017 -1.000000e+00 -1.836970e-16 -0.201299  0.979530   \n",
       "3 -2.449294e-16  1.000000  1.224647e-16 -1.000000e+00  0.651372 -0.758758   \n",
       "4  5.877853e-01 -0.809017 -2.449294e-16  1.000000e+00  0.201299  0.979530   \n",
       "\n",
       "   Group  Marital Status_NA  Customer Feedback_NA  Health Score_NA  \\\n",
       "0    195                  0                     0                0   \n",
       "1    169                  0                     0                0   \n",
       "2    184                  0                     0                0   \n",
       "3    217                  0                     0                0   \n",
       "4     96                  0                     0                0   \n",
       "\n",
       "   Previous Claims_NA  Vehicle Age_NA  Credit Score_NA  Insurance Duration_NA  \\\n",
       "0                   0               0                0                      0   \n",
       "1                   0               0                0                      0   \n",
       "2                   0               0                1                      0   \n",
       "3                   0               0                0                      0   \n",
       "4                   0               0                0                      0   \n",
       "\n",
       "   Occupation_Employed  Occupation_Self-Employed  Occupation_Unemployed  \\\n",
       "0                False                      True                  False   \n",
       "1                False                     False                  False   \n",
       "2                False                      True                  False   \n",
       "3                False                     False                  False   \n",
       "4                False                      True                  False   \n",
       "\n",
       "   Location_Rural  Location_Suburban  Location_Urban  Property Type_Apartment  \\\n",
       "0           False              False            True                    False   \n",
       "1            True              False           False                    False   \n",
       "2           False               True           False                    False   \n",
       "3            True              False           False                     True   \n",
       "4            True              False           False                    False   \n",
       "\n",
       "   Property Type_Condo  Property Type_House  Month_name_April  \\\n",
       "0                False                 True             False   \n",
       "1                False                 True             False   \n",
       "2                False                 True             False   \n",
       "3                False                False             False   \n",
       "4                False                 True             False   \n",
       "\n",
       "   Month_name_August  Month_name_December  Month_name_February  \\\n",
       "0              False                 True                False   \n",
       "1              False                False                False   \n",
       "2              False                False                False   \n",
       "3              False                False                False   \n",
       "4              False                 True                False   \n",
       "\n",
       "   Month_name_January  Month_name_July  Month_name_June  Month_name_March  \\\n",
       "0               False            False            False             False   \n",
       "1               False            False             True             False   \n",
       "2               False            False            False             False   \n",
       "3               False            False             True             False   \n",
       "4               False            False            False             False   \n",
       "\n",
       "   Month_name_May  Month_name_November  Month_name_October  \\\n",
       "0           False                False               False   \n",
       "1           False                False               False   \n",
       "2           False                False               False   \n",
       "3           False                False               False   \n",
       "4           False                False               False   \n",
       "\n",
       "   Month_name_September  Day_of_week_Friday  Day_of_week_Monday  \\\n",
       "0                 False               False               False   \n",
       "1                 False               False                True   \n",
       "2                  True               False               False   \n",
       "3                 False               False               False   \n",
       "4                 False               False               False   \n",
       "\n",
       "   Day_of_week_Saturday  Day_of_week_Sunday  Day_of_week_Thursday  \\\n",
       "0                  True               False                 False   \n",
       "1                 False               False                 False   \n",
       "2                  True               False                 False   \n",
       "3                 False               False                 False   \n",
       "4                 False               False                 False   \n",
       "\n",
       "   Day_of_week_Tuesday  Day_of_week_Wednesday  Income_Dependents Ratio  \\\n",
       "0                False                  False                   5024.5   \n",
       "1                False                  False                   7919.5   \n",
       "2                False                  False                   6400.5   \n",
       "3                False                   True                  47285.0   \n",
       "4                False                   True                  19825.5   \n",
       "\n",
       "   Income_per_Dependent  CreditScore_InsuranceDuration  Health_Risk_Score  \\\n",
       "0                5024.5                         1860.0           3.870062   \n",
       "1                7919.5                         1388.0           4.221513   \n",
       "2                6400.5                            NaN           2.641123   \n",
       "3               47285.0                          367.0           4.453093   \n",
       "4               19825.5                         2392.0           3.981195   \n",
       "\n",
       "   Credit_Health_Score  Health_Age_Interaction  contract_length  Age_Income  \n",
       "0          8406.738970              429.376453                2    190931.0  \n",
       "1         10805.393307              607.219509                1   1235442.0  \n",
       "2                  NaN             1085.083634                1    588846.0  \n",
       "3          4014.298906              229.701027                0   2978955.0  \n",
       "4         12184.903989              427.897966                2    832671.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200000, 70), (800000, 69))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.drop('Premium Amount', axis=1)\n",
    "y = train['Premium Amount']\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "n_splits=10\n",
    "folds = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "folds_train = KFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': 300,\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 10, 300),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 5, 12),\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#         'max_depth': trial.suggest_int('max_depth', -1, 12),\n",
    "#         'lambda_l1': trial.suggest_float('lambda_l1', 1e-4, 10.0),\n",
    "#         'lambda_l2': trial.suggest_float('lambda_l2', 1e-4, 10.0),\n",
    "#         'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.001, 0.1),\n",
    "#         'n_jobs': -1,\n",
    "#         'verbose': -1\n",
    "#     }\n",
    "\n",
    "#     model = LGBMRegressor(**params)\n",
    "#     scores = []\n",
    "\n",
    "#     for train_idx, val_idx in folds.split(x):\n",
    "#         x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
    "#         y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]\n",
    "\n",
    "#         model.fit(\n",
    "#             x_train, y_train, \n",
    "#             eval_set=[(x_val, y_val)],\n",
    "#             eval_metric='rmse',\n",
    "#             callbacks=[\n",
    "#                 early_stopping(50),\n",
    "#                 log_evaluation(10)\n",
    "#             ])\n",
    "#         preds = model.predict(x_val)\n",
    "#         score = np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(preds)))\n",
    "#         scores.append(score)\n",
    "#     return np.mean(scores)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=5)\n",
    "\n",
    "# best_params = study.best_params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 111,\n",
       " 'learning_rate': 0.0583357228522494,\n",
       " 'feature_fraction': 0.9607103509286345,\n",
       " 'bagging_fraction': 0.752032361770083,\n",
       " 'bagging_freq': 11,\n",
       " 'min_data_in_leaf': 74,\n",
       " 'max_depth': -1,\n",
       " 'lambda_l1': 4.094647896963407,\n",
       " 'lambda_l2': 7.310837913970018,\n",
       " 'min_gain_to_split': 0.004067559169870071,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3171\n",
      "[LightGBM] [Info] Number of data points in the train set: 960000, number of used features: 67\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 6.594502\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's rmse: 1.04741\tvalid_0's l2: 1.09706\n",
      "[100]\tvalid_0's rmse: 1.04715\tvalid_0's l2: 1.09651\n",
      "[150]\tvalid_0's rmse: 1.04713\tvalid_0's l2: 1.09648\n",
      "[200]\tvalid_0's rmse: 1.04724\tvalid_0's l2: 1.09671\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid_0's rmse: 1.0471\tvalid_0's l2: 1.09643\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3170\n",
      "[LightGBM] [Info] Number of data points in the train set: 960000, number of used features: 67\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 6.594469\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's rmse: 1.04823\tvalid_0's l2: 1.09878\n",
      "[100]\tvalid_0's rmse: 1.04802\tvalid_0's l2: 1.09835\n",
      "[150]\tvalid_0's rmse: 1.04813\tvalid_0's l2: 1.09857\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's rmse: 1.04798\tvalid_0's l2: 1.09825\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3170\n",
      "[LightGBM] [Info] Number of data points in the train set: 960000, number of used features: 67\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 6.594266\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's rmse: 1.04819\tvalid_0's l2: 1.09871\n",
      "[100]\tvalid_0's rmse: 1.04803\tvalid_0's l2: 1.09836\n",
      "[150]\tvalid_0's rmse: 1.04814\tvalid_0's l2: 1.0986\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's rmse: 1.04797\tvalid_0's l2: 1.09823\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3171\n",
      "[LightGBM] [Info] Number of data points in the train set: 960000, number of used features: 67\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 6.592996\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's rmse: 1.04447\tvalid_0's l2: 1.09092\n",
      "[100]\tvalid_0's rmse: 1.0441\tvalid_0's l2: 1.09015\n",
      "[150]\tvalid_0's rmse: 1.04419\tvalid_0's l2: 1.09034\n",
      "[200]\tvalid_0's rmse: 1.04426\tvalid_0's l2: 1.09049\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid_0's rmse: 1.04409\tvalid_0's l2: 1.09012\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3170\n",
      "[LightGBM] [Info] Number of data points in the train set: 960000, number of used features: 67\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 6.593212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's rmse: 1.04334\tvalid_0's l2: 1.08855\n",
      "[100]\tvalid_0's rmse: 1.04329\tvalid_0's l2: 1.08845\n",
      "[150]\tvalid_0's rmse: 1.04338\tvalid_0's l2: 1.08864\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's rmse: 1.04318\tvalid_0's l2: 1.08822\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607103509286345, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607103509286345\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.310837913970018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.310837913970018\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.004067559169870071, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.004067559169870071\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.094647896963407, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.094647896963407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.752032361770083, subsample=1.0 will be ignored. Current value: bagging_fraction=0.752032361770083\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "CPU times: user 4min 31s, sys: 2.19 s, total: 4min 33s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_params = {\n",
    " 'n_estimators': 500,\n",
    " 'boosting_type': 'gbdt',\n",
    " 'num_leaves': 111,\n",
    " 'learning_rate': 0.0583357228522494,\n",
    " 'feature_fraction': 0.9607103509286345,\n",
    " 'bagging_fraction': 0.752032361770083,\n",
    " 'bagging_freq': 11,\n",
    " 'min_data_in_leaf': 74,\n",
    " 'max_depth': -1,\n",
    " 'lambda_l1': 4.094647896963407,\n",
    " 'lambda_l2': 7.310837913970018,\n",
    " 'min_gain_to_split': 0.004067559169870071,\n",
    " 'n_jobs': -1\n",
    "}\n",
    "# best_params['n_estimators'] = 500\n",
    "\n",
    "models_lgb = []\n",
    "lgbm_OOF = np.zeros(len(x))\n",
    "lgbm_preds = np.zeros(len(test))\n",
    "\n",
    "for train_idx, val_idx in folds_train.split(x):\n",
    "    x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
    "    y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]\n",
    "\n",
    "    model = LGBMRegressor(**best_params)\n",
    "    model.fit(\n",
    "        x_train, y_train, \n",
    "        eval_set=[(x_val, y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[\n",
    "            early_stopping(100),\n",
    "            log_evaluation(50)\n",
    "        ])\n",
    "\n",
    "    lgbm_OOF[val_idx] += model.predict(x_val)\n",
    "    lgbm_preds += model.predict(test) / folds_train.n_splits\n",
    "    models_lgb.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.046063550028496\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation RMSE:\", np.sqrt(mean_squared_error(y_log, lgbm_OOF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"iterations\": 300,\n",
    "#         \"loss_function\": \"RMSE\",\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1),\n",
    "#         \"depth\": trial.suggest_int(\"depth\", 3, 12),\n",
    "#         \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-4, 10.0),\n",
    "#         \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 1e-3, 1.0),\n",
    "#         \"random_strength\": trial.suggest_float(\"random_strength\", 1e-3, 10.0),\n",
    "#         \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0),\n",
    "#         \"verbose\": 50,\n",
    "#         \"random_seed\": SEED,\n",
    "#     }\n",
    "\n",
    "#     scores = []\n",
    "#     for train_idx, val_idx in folds.split(x):\n",
    "#         x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
    "#         y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]\n",
    "\n",
    "#         model = CatBoostRegressor(**params)\n",
    "#         model.fit(\n",
    "#             x_train, y_train,\n",
    "#             eval_set=(x_val, y_val),\n",
    "#             early_stopping_rounds=50,\n",
    "#         )\n",
    "#         preds = model.predict(x_val)\n",
    "#         score = np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(preds)))\n",
    "#         scores.append(score)\n",
    "\n",
    "#     return np.mean(scores)\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=5)\n",
    "\n",
    "# best_params = study.best_params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0909730\ttest: 1.0927722\tbest: 1.0927722 (0)\ttotal: 66.2ms\tremaining: 1m 6s\n",
      "50:\tlearn: 1.0572052\ttest: 1.0593770\tbest: 1.0593770 (50)\ttotal: 3.64s\tremaining: 1m 7s\n",
      "100:\tlearn: 1.0545232\ttest: 1.0571008\tbest: 1.0571008 (100)\ttotal: 6.98s\tremaining: 1m 2s\n",
      "150:\tlearn: 1.0530146\ttest: 1.0560597\tbest: 1.0560597 (150)\ttotal: 10.1s\tremaining: 57s\n",
      "200:\tlearn: 1.0477474\ttest: 1.0509845\tbest: 1.0509845 (200)\ttotal: 13.2s\tremaining: 52.6s\n",
      "250:\tlearn: 1.0460310\ttest: 1.0498070\tbest: 1.0498070 (250)\ttotal: 16.6s\tremaining: 49.4s\n",
      "300:\tlearn: 1.0448203\ttest: 1.0493980\tbest: 1.0493980 (300)\ttotal: 19.9s\tremaining: 46.2s\n",
      "350:\tlearn: 1.0437929\ttest: 1.0491639\tbest: 1.0491619 (344)\ttotal: 23.3s\tremaining: 43.1s\n",
      "400:\tlearn: 1.0427509\ttest: 1.0489238\tbest: 1.0489188 (397)\ttotal: 26.8s\tremaining: 40s\n",
      "450:\tlearn: 1.0417748\ttest: 1.0487576\tbest: 1.0487576 (450)\ttotal: 30.2s\tremaining: 36.8s\n",
      "500:\tlearn: 1.0409085\ttest: 1.0487307\tbest: 1.0487204 (494)\ttotal: 33.6s\tremaining: 33.5s\n",
      "550:\tlearn: 1.0400777\ttest: 1.0487384\tbest: 1.0487204 (494)\ttotal: 36.9s\tremaining: 30.1s\n",
      "600:\tlearn: 1.0392055\ttest: 1.0487174\tbest: 1.0487073 (589)\ttotal: 40.3s\tremaining: 26.8s\n",
      "650:\tlearn: 1.0383522\ttest: 1.0486782\tbest: 1.0486595 (635)\ttotal: 43.9s\tremaining: 23.5s\n",
      "700:\tlearn: 1.0375342\ttest: 1.0487082\tbest: 1.0486595 (635)\ttotal: 47.3s\tremaining: 20.2s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 1.048659494\n",
      "bestIteration = 635\n",
      "\n",
      "Shrink model to first 636 iterations.\n",
      "0:\tlearn: 1.0910352\ttest: 1.0931915\tbest: 1.0931915 (0)\ttotal: 63.2ms\tremaining: 1m 3s\n",
      "50:\tlearn: 1.0574224\ttest: 1.0597872\tbest: 1.0597872 (50)\ttotal: 3.38s\tremaining: 1m 2s\n",
      "100:\tlearn: 1.0545251\ttest: 1.0573478\tbest: 1.0573478 (100)\ttotal: 6.76s\tremaining: 1m\n",
      "150:\tlearn: 1.0527437\ttest: 1.0560058\tbest: 1.0560058 (150)\ttotal: 10.1s\tremaining: 56.8s\n",
      "200:\tlearn: 1.0475993\ttest: 1.0513217\tbest: 1.0513217 (200)\ttotal: 13.4s\tremaining: 53.3s\n",
      "250:\tlearn: 1.0457614\ttest: 1.0502314\tbest: 1.0502314 (250)\ttotal: 16.8s\tremaining: 50s\n",
      "300:\tlearn: 1.0446075\ttest: 1.0499294\tbest: 1.0499294 (300)\ttotal: 20.2s\tremaining: 46.8s\n",
      "350:\tlearn: 1.0435308\ttest: 1.0496757\tbest: 1.0496757 (350)\ttotal: 23.5s\tremaining: 43.4s\n",
      "400:\tlearn: 1.0425494\ttest: 1.0496128\tbest: 1.0496017 (399)\ttotal: 26.8s\tremaining: 40s\n",
      "450:\tlearn: 1.0415759\ttest: 1.0495573\tbest: 1.0495315 (417)\ttotal: 30.1s\tremaining: 36.6s\n",
      "500:\tlearn: 1.0406536\ttest: 1.0494636\tbest: 1.0494570 (493)\ttotal: 33.5s\tremaining: 33.4s\n",
      "550:\tlearn: 1.0397893\ttest: 1.0493963\tbest: 1.0493963 (550)\ttotal: 36.8s\tremaining: 30s\n",
      "600:\tlearn: 1.0388818\ttest: 1.0493248\tbest: 1.0493216 (598)\ttotal: 40.2s\tremaining: 26.7s\n",
      "650:\tlearn: 1.0380108\ttest: 1.0493516\tbest: 1.0493125 (607)\ttotal: 43.5s\tremaining: 23.3s\n",
      "700:\tlearn: 1.0371813\ttest: 1.0493624\tbest: 1.0493125 (607)\ttotal: 46.8s\tremaining: 20s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 1.049312492\n",
      "bestIteration = 607\n",
      "\n",
      "Shrink model to first 608 iterations.\n",
      "0:\tlearn: 1.0909103\ttest: 1.0930851\tbest: 1.0930851 (0)\ttotal: 64ms\tremaining: 1m 3s\n",
      "50:\tlearn: 1.0566676\ttest: 1.0595485\tbest: 1.0595485 (50)\ttotal: 3.49s\tremaining: 1m 4s\n",
      "100:\tlearn: 1.0545904\ttest: 1.0579931\tbest: 1.0579931 (100)\ttotal: 6.83s\tremaining: 1m\n",
      "150:\tlearn: 1.0525236\ttest: 1.0564854\tbest: 1.0564854 (150)\ttotal: 10.2s\tremaining: 57.3s\n",
      "200:\tlearn: 1.0474490\ttest: 1.0516690\tbest: 1.0516690 (200)\ttotal: 13.5s\tremaining: 53.7s\n",
      "250:\tlearn: 1.0455811\ttest: 1.0503404\tbest: 1.0503404 (250)\ttotal: 16.9s\tremaining: 50.5s\n",
      "300:\tlearn: 1.0443600\ttest: 1.0498548\tbest: 1.0498538 (298)\ttotal: 20.3s\tremaining: 47.2s\n",
      "350:\tlearn: 1.0433498\ttest: 1.0497070\tbest: 1.0496908 (340)\ttotal: 23.6s\tremaining: 43.7s\n",
      "400:\tlearn: 1.0423908\ttest: 1.0495787\tbest: 1.0495774 (399)\ttotal: 27s\tremaining: 40.3s\n",
      "450:\tlearn: 1.0414267\ttest: 1.0494635\tbest: 1.0494635 (450)\ttotal: 30.3s\tremaining: 36.8s\n",
      "500:\tlearn: 1.0405923\ttest: 1.0494580\tbest: 1.0494196 (472)\ttotal: 33.8s\tremaining: 33.6s\n",
      "550:\tlearn: 1.0397084\ttest: 1.0494322\tbest: 1.0494196 (472)\ttotal: 37.1s\tremaining: 30.3s\n",
      "600:\tlearn: 1.0388763\ttest: 1.0494185\tbest: 1.0494100 (568)\ttotal: 40.4s\tremaining: 26.8s\n",
      "650:\tlearn: 1.0380474\ttest: 1.0494454\tbest: 1.0494100 (568)\ttotal: 43.7s\tremaining: 23.4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 1.049409961\n",
      "bestIteration = 568\n",
      "\n",
      "Shrink model to first 569 iterations.\n",
      "0:\tlearn: 1.0913055\ttest: 1.0893971\tbest: 1.0893971 (0)\ttotal: 63ms\tremaining: 1m 2s\n",
      "50:\tlearn: 1.0573686\ttest: 1.0557572\tbest: 1.0557572 (50)\ttotal: 3.42s\tremaining: 1m 3s\n",
      "100:\tlearn: 1.0547542\ttest: 1.0534966\tbest: 1.0534966 (100)\ttotal: 6.87s\tremaining: 1m 1s\n",
      "150:\tlearn: 1.0533752\ttest: 1.0525203\tbest: 1.0525203 (150)\ttotal: 10.3s\tremaining: 57.7s\n",
      "200:\tlearn: 1.0488837\ttest: 1.0483182\tbest: 1.0483182 (200)\ttotal: 13.7s\tremaining: 54.5s\n",
      "250:\tlearn: 1.0470894\ttest: 1.0470905\tbest: 1.0470905 (250)\ttotal: 17s\tremaining: 50.7s\n",
      "300:\tlearn: 1.0458670\ttest: 1.0466536\tbest: 1.0466536 (300)\ttotal: 20.4s\tremaining: 47.4s\n",
      "350:\tlearn: 1.0448845\ttest: 1.0464124\tbest: 1.0464124 (350)\ttotal: 23.7s\tremaining: 43.9s\n",
      "400:\tlearn: 1.0438618\ttest: 1.0461798\tbest: 1.0461798 (400)\ttotal: 27s\tremaining: 40.3s\n",
      "450:\tlearn: 1.0429701\ttest: 1.0461144\tbest: 1.0461102 (447)\ttotal: 30.3s\tremaining: 36.9s\n",
      "500:\tlearn: 1.0420285\ttest: 1.0459804\tbest: 1.0459776 (499)\ttotal: 33.8s\tremaining: 33.6s\n",
      "550:\tlearn: 1.0411671\ttest: 1.0459261\tbest: 1.0459249 (548)\ttotal: 37.2s\tremaining: 30.4s\n",
      "600:\tlearn: 1.0403325\ttest: 1.0459147\tbest: 1.0458941 (568)\ttotal: 40.6s\tremaining: 26.9s\n",
      "650:\tlearn: 1.0394995\ttest: 1.0459172\tbest: 1.0458941 (568)\ttotal: 43.8s\tremaining: 23.5s\n",
      "700:\tlearn: 1.0386461\ttest: 1.0459195\tbest: 1.0458855 (664)\ttotal: 47.2s\tremaining: 20.1s\n",
      "750:\tlearn: 1.0378347\ttest: 1.0459720\tbest: 1.0458855 (664)\ttotal: 50.4s\tremaining: 16.7s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 1.045885468\n",
      "bestIteration = 664\n",
      "\n",
      "Shrink model to first 665 iterations.\n",
      "0:\tlearn: 1.0918027\ttest: 1.0875864\tbest: 1.0875864 (0)\ttotal: 65.3ms\tremaining: 1m 5s\n",
      "50:\tlearn: 1.0572987\ttest: 1.0538660\tbest: 1.0538660 (50)\ttotal: 3.45s\tremaining: 1m 4s\n",
      "100:\tlearn: 1.0547868\ttest: 1.0519277\tbest: 1.0519277 (100)\ttotal: 6.84s\tremaining: 1m\n",
      "150:\tlearn: 1.0531472\ttest: 1.0508658\tbest: 1.0508658 (150)\ttotal: 10.3s\tremaining: 57.8s\n",
      "200:\tlearn: 1.0482819\ttest: 1.0465303\tbest: 1.0465303 (200)\ttotal: 13.7s\tremaining: 54.5s\n",
      "250:\tlearn: 1.0466048\ttest: 1.0454753\tbest: 1.0454753 (250)\ttotal: 17.1s\tremaining: 51s\n",
      "300:\tlearn: 1.0455150\ttest: 1.0451668\tbest: 1.0451668 (300)\ttotal: 20.6s\tremaining: 47.7s\n",
      "350:\tlearn: 1.0444642\ttest: 1.0448882\tbest: 1.0448819 (347)\ttotal: 23.9s\tremaining: 44.1s\n",
      "400:\tlearn: 1.0434329\ttest: 1.0447467\tbest: 1.0447441 (397)\ttotal: 27.2s\tremaining: 40.6s\n",
      "450:\tlearn: 1.0424640\ttest: 1.0446819\tbest: 1.0446738 (441)\ttotal: 30.5s\tremaining: 37.1s\n",
      "500:\tlearn: 1.0415337\ttest: 1.0446677\tbest: 1.0446408 (492)\ttotal: 33.8s\tremaining: 33.7s\n",
      "550:\tlearn: 1.0406957\ttest: 1.0446741\tbest: 1.0446408 (492)\ttotal: 37.3s\tremaining: 30.4s\n",
      "600:\tlearn: 1.0398209\ttest: 1.0446137\tbest: 1.0446107 (594)\ttotal: 40.6s\tremaining: 27s\n",
      "650:\tlearn: 1.0389852\ttest: 1.0446284\tbest: 1.0446034 (613)\ttotal: 44.1s\tremaining: 23.7s\n",
      "700:\tlearn: 1.0381717\ttest: 1.0446295\tbest: 1.0446034 (613)\ttotal: 47.5s\tremaining: 20.3s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 1.044603385\n",
      "bestIteration = 613\n",
      "\n",
      "Shrink model to first 614 iterations.\n",
      "CPU times: user 14min 15s, sys: 14.4 s, total: 14min 30s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_params = {\n",
    " 'iterations':1000,\n",
    " 'loss_function': 'RMSE',\n",
    " 'learning_rate': 0.09295757892732069,\n",
    " 'depth': 7,\n",
    " 'l2_leaf_reg': 2.8780706448862734,\n",
    " 'bagging_temperature': 0.12215801350190825,\n",
    " 'random_strength': 8.553048856390589,\n",
    " 'border_count': 232,\n",
    " 'colsample_bylevel': 0.7252465177667906,\n",
    " 'verbose': 50,\n",
    " 'random_seed': SEED,\n",
    "}\n",
    "\n",
    "# best_params['iterations'] = 500\n",
    "\n",
    "models_cat = []\n",
    "cat_OOF = np.zeros(len(x))\n",
    "cat_preds = np.zeros(len(test))\n",
    "\n",
    "for train_idx, val_idx in folds_train.split(x):\n",
    "    x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
    "    y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        **best_params,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=(x_val, y_val),\n",
    "        early_stopping_rounds=100,\n",
    "    )\n",
    "\n",
    "    cat_OOF[val_idx] += model.predict(x_val)\n",
    "    cat_preds += model.predict(test) / folds_train.n_splits\n",
    "    models_cat.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.0475759976811865\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation RMSE:\", np.sqrt(mean_squared_error(y_log, cat_OOF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import set_config\n",
    "\n",
    "#   \n",
    "set_config(verbosity=3)  #     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"eval_metric\": \"rmse\",\n",
    "#         \"booster\": \"gbtree\",\n",
    "#         \"eta\": trial.suggest_float(\"eta\", 1e-4, 1e-1, log=True),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "#         \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-4, 10.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 0.001, 0.1),\n",
    "#         \"seed\": SEED,\n",
    "#         \"verbosity\": 3\n",
    "#     }\n",
    "\n",
    "#     scores = []\n",
    "#     for train_idx, val_idx in folds.split(x):\n",
    "#         x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
    "#         y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]\n",
    "\n",
    "#         model = XGBRegressor(**params)\n",
    "#         # early_stop = callback.EarlyStopping(rounds=50, metric_name='rmse', save_best=True)\n",
    "#         model.fit(\n",
    "#             x_train, y_train,\n",
    "#             eval_set=[(x_val, y_val)],\n",
    "#             # callbacks=[early_stop],\n",
    "#             verbose=True\n",
    "#         )\n",
    "#         preds = model.predict(x_val)\n",
    "#         score = np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(preds)))\n",
    "#         scores.append(score)\n",
    "\n",
    "#     return np.mean(scores)\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=5)\n",
    "\n",
    "# best_params = study.best_params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:12:26] ======== Monitor (0): HostSketchContainer ========\n",
      "[11:12:26] AllReduce: 0.016872s, 1 calls @ 16872us\n",
      "\n",
      "[11:12:26] MakeCuts: 0.01726s, 1 calls @ 17260us\n",
      "\n",
      "[11:12:27] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[0]\tvalidation_0-rmse:1.09523\n",
      "[1]\tvalidation_0-rmse:1.09369\n",
      "[2]\tvalidation_0-rmse:1.09140\n",
      "[3]\tvalidation_0-rmse:1.08973\n",
      "[4]\tvalidation_0-rmse:1.08757\n",
      "[5]\tvalidation_0-rmse:1.08607\n",
      "[6]\tvalidation_0-rmse:1.08466\n",
      "[7]\tvalidation_0-rmse:1.08276\n",
      "[8]\tvalidation_0-rmse:1.08182\n",
      "[9]\tvalidation_0-rmse:1.08007\n",
      "[10]\tvalidation_0-rmse:1.07885\n",
      "[11]\tvalidation_0-rmse:1.07757\n",
      "[12]\tvalidation_0-rmse:1.07603\n",
      "[13]\tvalidation_0-rmse:1.07457\n",
      "[14]\tvalidation_0-rmse:1.07342\n",
      "[15]\tvalidation_0-rmse:1.07209\n",
      "[16]\tvalidation_0-rmse:1.07083\n",
      "[17]\tvalidation_0-rmse:1.06997\n",
      "[18]\tvalidation_0-rmse:1.06882\n",
      "[19]\tvalidation_0-rmse:1.06797\n",
      "[20]\tvalidation_0-rmse:1.06692\n",
      "[21]\tvalidation_0-rmse:1.06602\n",
      "[22]\tvalidation_0-rmse:1.06506\n",
      "[23]\tvalidation_0-rmse:1.06434\n",
      "[24]\tvalidation_0-rmse:1.06367\n",
      "[25]\tvalidation_0-rmse:1.06332\n",
      "[26]\tvalidation_0-rmse:1.06288\n",
      "[27]\tvalidation_0-rmse:1.06208\n",
      "[28]\tvalidation_0-rmse:1.06138\n",
      "[29]\tvalidation_0-rmse:1.06066\n",
      "[30]\tvalidation_0-rmse:1.06000\n",
      "[31]\tvalidation_0-rmse:1.05954\n",
      "[32]\tvalidation_0-rmse:1.05907\n",
      "[33]\tvalidation_0-rmse:1.05875\n",
      "[34]\tvalidation_0-rmse:1.05835\n",
      "[35]\tvalidation_0-rmse:1.05778\n",
      "[36]\tvalidation_0-rmse:1.05744\n",
      "[37]\tvalidation_0-rmse:1.05726\n",
      "[38]\tvalidation_0-rmse:1.05688\n",
      "[39]\tvalidation_0-rmse:1.05669\n",
      "[40]\tvalidation_0-rmse:1.05645\n",
      "[41]\tvalidation_0-rmse:1.05598\n",
      "[42]\tvalidation_0-rmse:1.05578\n",
      "[43]\tvalidation_0-rmse:1.05534\n",
      "[44]\tvalidation_0-rmse:1.05503\n",
      "[45]\tvalidation_0-rmse:1.05464\n",
      "[46]\tvalidation_0-rmse:1.05428\n",
      "[47]\tvalidation_0-rmse:1.05392\n",
      "[48]\tvalidation_0-rmse:1.05375\n",
      "[49]\tvalidation_0-rmse:1.05345\n",
      "[50]\tvalidation_0-rmse:1.05322\n",
      "[51]\tvalidation_0-rmse:1.05299\n",
      "[52]\tvalidation_0-rmse:1.05280\n",
      "[53]\tvalidation_0-rmse:1.05261\n",
      "[54]\tvalidation_0-rmse:1.05234\n",
      "[55]\tvalidation_0-rmse:1.05208\n",
      "[56]\tvalidation_0-rmse:1.05196\n",
      "[57]\tvalidation_0-rmse:1.05181\n",
      "[58]\tvalidation_0-rmse:1.05165\n",
      "[59]\tvalidation_0-rmse:1.05152\n",
      "[60]\tvalidation_0-rmse:1.05137\n",
      "[61]\tvalidation_0-rmse:1.05123\n",
      "[62]\tvalidation_0-rmse:1.05109\n",
      "[63]\tvalidation_0-rmse:1.05096\n",
      "[64]\tvalidation_0-rmse:1.05076\n",
      "[65]\tvalidation_0-rmse:1.05069\n",
      "[66]\tvalidation_0-rmse:1.05058\n",
      "[67]\tvalidation_0-rmse:1.05048\n",
      "[68]\tvalidation_0-rmse:1.05041\n",
      "[69]\tvalidation_0-rmse:1.05030\n",
      "[70]\tvalidation_0-rmse:1.05019\n",
      "[71]\tvalidation_0-rmse:1.05005\n",
      "[72]\tvalidation_0-rmse:1.04998\n",
      "[73]\tvalidation_0-rmse:1.04992\n",
      "[74]\tvalidation_0-rmse:1.04982\n",
      "[75]\tvalidation_0-rmse:1.04969\n",
      "[76]\tvalidation_0-rmse:1.04955\n",
      "[77]\tvalidation_0-rmse:1.04946\n",
      "[78]\tvalidation_0-rmse:1.04940\n",
      "[79]\tvalidation_0-rmse:1.04934\n",
      "[80]\tvalidation_0-rmse:1.04922\n",
      "[81]\tvalidation_0-rmse:1.04912\n",
      "[82]\tvalidation_0-rmse:1.04908\n",
      "[83]\tvalidation_0-rmse:1.04901\n",
      "[84]\tvalidation_0-rmse:1.04891\n",
      "[85]\tvalidation_0-rmse:1.04880\n",
      "[86]\tvalidation_0-rmse:1.04872\n",
      "[87]\tvalidation_0-rmse:1.04863\n",
      "[88]\tvalidation_0-rmse:1.04858\n",
      "[89]\tvalidation_0-rmse:1.04854\n",
      "[90]\tvalidation_0-rmse:1.04851\n",
      "[91]\tvalidation_0-rmse:1.04842\n",
      "[92]\tvalidation_0-rmse:1.04835\n",
      "[93]\tvalidation_0-rmse:1.04831\n",
      "[94]\tvalidation_0-rmse:1.04829\n",
      "[95]\tvalidation_0-rmse:1.04825\n",
      "[96]\tvalidation_0-rmse:1.04819\n",
      "[97]\tvalidation_0-rmse:1.04818\n",
      "[98]\tvalidation_0-rmse:1.04813\n",
      "[99]\tvalidation_0-rmse:1.04811\n",
      "[11:12:46] ======== Monitor (0): Learner ========\n",
      "[11:12:46] Configure: 0.011259s, 1 calls @ 11259us\n",
      "\n",
      "[11:12:46] EvalOneIter: 7.87551s, 100 calls @ 7875506us\n",
      "\n",
      "[11:12:46] GetGradient: 0.09859s, 100 calls @ 98590us\n",
      "\n",
      "[11:12:46] PredictRaw: 0.000759s, 100 calls @ 759us\n",
      "\n",
      "[11:12:46] UpdateOneIter: 10.989s, 100 calls @ 10988957us\n",
      "\n",
      "[11:12:46] ======== Monitor (0): GBTree ========\n",
      "[11:12:46] BoostNewTrees: 10.865s, 100 calls @ 10865021us\n",
      "\n",
      "[11:12:46] CommitModel: 0.00018s, 100 calls @ 180us\n",
      "\n",
      "[11:12:46] ======== Monitor (0): HistUpdater ========\n",
      "[11:12:46] BuildHistogram: 6.26672s, 700 calls @ 6266718us\n",
      "\n",
      "[11:12:46] EvaluateSplits: 0.497259s, 800 calls @ 497259us\n",
      "\n",
      "[11:12:46] InitData: 0.115805s, 100 calls @ 115805us\n",
      "\n",
      "[11:12:46] InitRoot: 1.84205s, 100 calls @ 1842048us\n",
      "\n",
      "[11:12:46] LeafPartition: 3.1e-05s, 100 calls @ 31us\n",
      "\n",
      "[11:12:46] UpdatePosition: 1.90371s, 800 calls @ 1903714us\n",
      "\n",
      "[11:12:46] UpdatePredictionCache: 0.136084s, 100 calls @ 136084us\n",
      "\n",
      "[11:12:46] UpdateTree: 10.6337s, 100 calls @ 10633737us\n",
      "\n",
      "[11:12:46] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[11:12:51] ======== Monitor (0): HostSketchContainer ========\n",
      "[11:12:51] AllReduce: 0.016187s, 1 calls @ 16187us\n",
      "\n",
      "[11:12:51] MakeCuts: 0.016251s, 1 calls @ 16251us\n",
      "\n",
      "[11:12:51] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[0]\tvalidation_0-rmse:1.09548\n",
      "[1]\tvalidation_0-rmse:1.09396\n",
      "[2]\tvalidation_0-rmse:1.09169\n",
      "[3]\tvalidation_0-rmse:1.09003\n",
      "[4]\tvalidation_0-rmse:1.08790\n",
      "[5]\tvalidation_0-rmse:1.08642\n",
      "[6]\tvalidation_0-rmse:1.08501\n",
      "[7]\tvalidation_0-rmse:1.08313\n",
      "[8]\tvalidation_0-rmse:1.08220\n",
      "[9]\tvalidation_0-rmse:1.08047\n",
      "[10]\tvalidation_0-rmse:1.07926\n",
      "[11]\tvalidation_0-rmse:1.07803\n",
      "[12]\tvalidation_0-rmse:1.07649\n",
      "[13]\tvalidation_0-rmse:1.07504\n",
      "[14]\tvalidation_0-rmse:1.07388\n",
      "[15]\tvalidation_0-rmse:1.07256\n",
      "[16]\tvalidation_0-rmse:1.07131\n",
      "[17]\tvalidation_0-rmse:1.07047\n",
      "[18]\tvalidation_0-rmse:1.06932\n",
      "[19]\tvalidation_0-rmse:1.06849\n",
      "[20]\tvalidation_0-rmse:1.06745\n",
      "[21]\tvalidation_0-rmse:1.06655\n",
      "[22]\tvalidation_0-rmse:1.06561\n",
      "[23]\tvalidation_0-rmse:1.06488\n",
      "[24]\tvalidation_0-rmse:1.06420\n",
      "[25]\tvalidation_0-rmse:1.06385\n",
      "[26]\tvalidation_0-rmse:1.06341\n",
      "[27]\tvalidation_0-rmse:1.06263\n",
      "[28]\tvalidation_0-rmse:1.06192\n",
      "[29]\tvalidation_0-rmse:1.06121\n",
      "[30]\tvalidation_0-rmse:1.06057\n",
      "[31]\tvalidation_0-rmse:1.06010\n",
      "[32]\tvalidation_0-rmse:1.05962\n",
      "[33]\tvalidation_0-rmse:1.05928\n",
      "[34]\tvalidation_0-rmse:1.05889\n",
      "[35]\tvalidation_0-rmse:1.05834\n",
      "[36]\tvalidation_0-rmse:1.05804\n",
      "[37]\tvalidation_0-rmse:1.05784\n",
      "[38]\tvalidation_0-rmse:1.05751\n",
      "[39]\tvalidation_0-rmse:1.05731\n",
      "[40]\tvalidation_0-rmse:1.05706\n",
      "[41]\tvalidation_0-rmse:1.05660\n",
      "[42]\tvalidation_0-rmse:1.05639\n",
      "[43]\tvalidation_0-rmse:1.05597\n",
      "[44]\tvalidation_0-rmse:1.05568\n",
      "[45]\tvalidation_0-rmse:1.05531\n",
      "[46]\tvalidation_0-rmse:1.05496\n",
      "[47]\tvalidation_0-rmse:1.05462\n",
      "[48]\tvalidation_0-rmse:1.05442\n",
      "[49]\tvalidation_0-rmse:1.05412\n",
      "[50]\tvalidation_0-rmse:1.05389\n",
      "[51]\tvalidation_0-rmse:1.05367\n",
      "[52]\tvalidation_0-rmse:1.05347\n",
      "[53]\tvalidation_0-rmse:1.05329\n",
      "[54]\tvalidation_0-rmse:1.05303\n",
      "[55]\tvalidation_0-rmse:1.05277\n",
      "[56]\tvalidation_0-rmse:1.05265\n",
      "[57]\tvalidation_0-rmse:1.05252\n",
      "[58]\tvalidation_0-rmse:1.05236\n",
      "[59]\tvalidation_0-rmse:1.05224\n",
      "[60]\tvalidation_0-rmse:1.05208\n",
      "[61]\tvalidation_0-rmse:1.05194\n",
      "[62]\tvalidation_0-rmse:1.05180\n",
      "[63]\tvalidation_0-rmse:1.05167\n",
      "[64]\tvalidation_0-rmse:1.05149\n",
      "[65]\tvalidation_0-rmse:1.05142\n",
      "[66]\tvalidation_0-rmse:1.05132\n",
      "[67]\tvalidation_0-rmse:1.05123\n",
      "[68]\tvalidation_0-rmse:1.05115\n",
      "[69]\tvalidation_0-rmse:1.05103\n",
      "[70]\tvalidation_0-rmse:1.05093\n",
      "[71]\tvalidation_0-rmse:1.05079\n",
      "[72]\tvalidation_0-rmse:1.05072\n",
      "[73]\tvalidation_0-rmse:1.05066\n",
      "[74]\tvalidation_0-rmse:1.05058\n",
      "[75]\tvalidation_0-rmse:1.05045\n",
      "[76]\tvalidation_0-rmse:1.05031\n",
      "[77]\tvalidation_0-rmse:1.05025\n",
      "[78]\tvalidation_0-rmse:1.05019\n",
      "[79]\tvalidation_0-rmse:1.05013\n",
      "[80]\tvalidation_0-rmse:1.05002\n",
      "[81]\tvalidation_0-rmse:1.04993\n",
      "[82]\tvalidation_0-rmse:1.04989\n",
      "[83]\tvalidation_0-rmse:1.04982\n",
      "[84]\tvalidation_0-rmse:1.04973\n",
      "[85]\tvalidation_0-rmse:1.04963\n",
      "[86]\tvalidation_0-rmse:1.04955\n",
      "[87]\tvalidation_0-rmse:1.04947\n",
      "[88]\tvalidation_0-rmse:1.04943\n",
      "[89]\tvalidation_0-rmse:1.04938\n",
      "[90]\tvalidation_0-rmse:1.04935\n",
      "[91]\tvalidation_0-rmse:1.04927\n",
      "[92]\tvalidation_0-rmse:1.04920\n",
      "[93]\tvalidation_0-rmse:1.04916\n",
      "[94]\tvalidation_0-rmse:1.04914\n",
      "[95]\tvalidation_0-rmse:1.04911\n",
      "[96]\tvalidation_0-rmse:1.04904\n",
      "[97]\tvalidation_0-rmse:1.04902\n",
      "[98]\tvalidation_0-rmse:1.04898\n",
      "[99]\tvalidation_0-rmse:1.04897\n",
      "[11:13:10] ======== Monitor (0): Learner ========\n",
      "[11:13:10] Configure: 0.000691s, 1 calls @ 691us\n",
      "\n",
      "[11:13:10] EvalOneIter: 7.7505s, 100 calls @ 7750500us\n",
      "\n",
      "[11:13:10] GetGradient: 0.141389s, 100 calls @ 141389us\n",
      "\n",
      "[11:13:10] PredictRaw: 0.000751s, 100 calls @ 751us\n",
      "\n",
      "[11:13:10] UpdateOneIter: 10.7824s, 100 calls @ 10782369us\n",
      "\n",
      "[11:13:10] ======== Monitor (0): GBTree ========\n",
      "[11:13:10] BoostNewTrees: 10.6285s, 100 calls @ 10628459us\n",
      "\n",
      "[11:13:10] CommitModel: 0.000148s, 100 calls @ 148us\n",
      "\n",
      "[11:13:10] ======== Monitor (0): HistUpdater ========\n",
      "[11:13:10] BuildHistogram: 6.00075s, 700 calls @ 6000754us\n",
      "\n",
      "[11:13:10] EvaluateSplits: 0.460307s, 800 calls @ 460307us\n",
      "\n",
      "[11:13:10] InitData: 0.154821s, 100 calls @ 154821us\n",
      "\n",
      "[11:13:10] InitRoot: 1.83193s, 100 calls @ 1831927us\n",
      "\n",
      "[11:13:10] LeafPartition: 3.6e-05s, 100 calls @ 36us\n",
      "\n",
      "[11:13:10] UpdatePosition: 1.90901s, 800 calls @ 1909008us\n",
      "\n",
      "[11:13:10] UpdatePredictionCache: 0.143998s, 100 calls @ 143998us\n",
      "\n",
      "[11:13:10] UpdateTree: 10.3492s, 100 calls @ 10349212us\n",
      "\n",
      "[11:13:10] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[11:13:14] ======== Monitor (0): HostSketchContainer ========\n",
      "[11:13:14] AllReduce: 0.016304s, 1 calls @ 16304us\n",
      "\n",
      "[11:13:14] MakeCuts: 0.016368s, 1 calls @ 16368us\n",
      "\n",
      "[11:13:15] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[0]\tvalidation_0-rmse:1.09549\n",
      "[1]\tvalidation_0-rmse:1.09396\n",
      "[2]\tvalidation_0-rmse:1.09168\n",
      "[3]\tvalidation_0-rmse:1.09002\n",
      "[4]\tvalidation_0-rmse:1.08789\n",
      "[5]\tvalidation_0-rmse:1.08637\n",
      "[6]\tvalidation_0-rmse:1.08497\n",
      "[7]\tvalidation_0-rmse:1.08310\n",
      "[8]\tvalidation_0-rmse:1.08215\n",
      "[9]\tvalidation_0-rmse:1.08041\n",
      "[10]\tvalidation_0-rmse:1.07922\n",
      "[11]\tvalidation_0-rmse:1.07800\n",
      "[12]\tvalidation_0-rmse:1.07647\n",
      "[13]\tvalidation_0-rmse:1.07501\n",
      "[14]\tvalidation_0-rmse:1.07386\n",
      "[15]\tvalidation_0-rmse:1.07254\n",
      "[16]\tvalidation_0-rmse:1.07128\n",
      "[17]\tvalidation_0-rmse:1.07043\n",
      "[18]\tvalidation_0-rmse:1.06930\n",
      "[19]\tvalidation_0-rmse:1.06846\n",
      "[20]\tvalidation_0-rmse:1.06742\n",
      "[21]\tvalidation_0-rmse:1.06653\n",
      "[22]\tvalidation_0-rmse:1.06559\n",
      "[23]\tvalidation_0-rmse:1.06487\n",
      "[24]\tvalidation_0-rmse:1.06419\n",
      "[25]\tvalidation_0-rmse:1.06386\n",
      "[26]\tvalidation_0-rmse:1.06343\n",
      "[27]\tvalidation_0-rmse:1.06264\n",
      "[28]\tvalidation_0-rmse:1.06193\n",
      "[29]\tvalidation_0-rmse:1.06123\n",
      "[30]\tvalidation_0-rmse:1.06057\n",
      "[31]\tvalidation_0-rmse:1.06011\n",
      "[32]\tvalidation_0-rmse:1.05963\n",
      "[33]\tvalidation_0-rmse:1.05931\n",
      "[34]\tvalidation_0-rmse:1.05891\n",
      "[35]\tvalidation_0-rmse:1.05835\n",
      "[36]\tvalidation_0-rmse:1.05803\n",
      "[37]\tvalidation_0-rmse:1.05784\n",
      "[38]\tvalidation_0-rmse:1.05750\n",
      "[39]\tvalidation_0-rmse:1.05730\n",
      "[40]\tvalidation_0-rmse:1.05707\n",
      "[41]\tvalidation_0-rmse:1.05661\n",
      "[42]\tvalidation_0-rmse:1.05641\n",
      "[43]\tvalidation_0-rmse:1.05598\n",
      "[44]\tvalidation_0-rmse:1.05567\n",
      "[45]\tvalidation_0-rmse:1.05529\n",
      "[46]\tvalidation_0-rmse:1.05495\n",
      "[47]\tvalidation_0-rmse:1.05461\n",
      "[48]\tvalidation_0-rmse:1.05441\n",
      "[49]\tvalidation_0-rmse:1.05411\n",
      "[50]\tvalidation_0-rmse:1.05388\n",
      "[51]\tvalidation_0-rmse:1.05366\n",
      "[52]\tvalidation_0-rmse:1.05346\n",
      "[53]\tvalidation_0-rmse:1.05329\n",
      "[54]\tvalidation_0-rmse:1.05302\n",
      "[55]\tvalidation_0-rmse:1.05276\n",
      "[56]\tvalidation_0-rmse:1.05265\n",
      "[57]\tvalidation_0-rmse:1.05251\n",
      "[58]\tvalidation_0-rmse:1.05235\n",
      "[59]\tvalidation_0-rmse:1.05222\n",
      "[60]\tvalidation_0-rmse:1.05206\n",
      "[61]\tvalidation_0-rmse:1.05194\n",
      "[62]\tvalidation_0-rmse:1.05179\n",
      "[63]\tvalidation_0-rmse:1.05167\n",
      "[64]\tvalidation_0-rmse:1.05148\n",
      "[65]\tvalidation_0-rmse:1.05142\n",
      "[66]\tvalidation_0-rmse:1.05133\n",
      "[67]\tvalidation_0-rmse:1.05122\n",
      "[68]\tvalidation_0-rmse:1.05115\n",
      "[69]\tvalidation_0-rmse:1.05105\n",
      "[70]\tvalidation_0-rmse:1.05094\n",
      "[71]\tvalidation_0-rmse:1.05079\n",
      "[72]\tvalidation_0-rmse:1.05072\n",
      "[73]\tvalidation_0-rmse:1.05067\n",
      "[74]\tvalidation_0-rmse:1.05058\n",
      "[75]\tvalidation_0-rmse:1.05043\n",
      "[76]\tvalidation_0-rmse:1.05029\n",
      "[77]\tvalidation_0-rmse:1.05023\n",
      "[78]\tvalidation_0-rmse:1.05017\n",
      "[79]\tvalidation_0-rmse:1.05010\n",
      "[80]\tvalidation_0-rmse:1.04999\n",
      "[81]\tvalidation_0-rmse:1.04989\n",
      "[82]\tvalidation_0-rmse:1.04985\n",
      "[83]\tvalidation_0-rmse:1.04978\n",
      "[84]\tvalidation_0-rmse:1.04968\n",
      "[85]\tvalidation_0-rmse:1.04958\n",
      "[86]\tvalidation_0-rmse:1.04950\n",
      "[87]\tvalidation_0-rmse:1.04941\n",
      "[88]\tvalidation_0-rmse:1.04938\n",
      "[89]\tvalidation_0-rmse:1.04933\n",
      "[90]\tvalidation_0-rmse:1.04930\n",
      "[91]\tvalidation_0-rmse:1.04922\n",
      "[92]\tvalidation_0-rmse:1.04914\n",
      "[93]\tvalidation_0-rmse:1.04911\n",
      "[94]\tvalidation_0-rmse:1.04909\n",
      "[95]\tvalidation_0-rmse:1.04906\n",
      "[96]\tvalidation_0-rmse:1.04900\n",
      "[97]\tvalidation_0-rmse:1.04898\n",
      "[98]\tvalidation_0-rmse:1.04894\n",
      "[99]\tvalidation_0-rmse:1.04893\n",
      "[11:13:34] ======== Monitor (0): Learner ========\n",
      "[11:13:34] Configure: 0.000755s, 1 calls @ 755us\n",
      "\n",
      "[11:13:34] EvalOneIter: 7.89712s, 100 calls @ 7897120us\n",
      "\n",
      "[11:13:34] GetGradient: 0.125282s, 100 calls @ 125282us\n",
      "\n",
      "[11:13:34] PredictRaw: 0.000783s, 100 calls @ 783us\n",
      "\n",
      "[11:13:34] UpdateOneIter: 11.0215s, 100 calls @ 11021457us\n",
      "\n",
      "[11:13:34] ======== Monitor (0): GBTree ========\n",
      "[11:13:34] BoostNewTrees: 10.883s, 100 calls @ 10882968us\n",
      "\n",
      "[11:13:34] CommitModel: 0.000136s, 100 calls @ 136us\n",
      "\n",
      "[11:13:34] ======== Monitor (0): HistUpdater ========\n",
      "[11:13:34] BuildHistogram: 6.19663s, 700 calls @ 6196627us\n",
      "\n",
      "[11:13:34] EvaluateSplits: 0.485869s, 800 calls @ 485869us\n",
      "\n",
      "[11:13:34] InitData: 0.153131s, 100 calls @ 153131us\n",
      "\n",
      "[11:13:34] InitRoot: 1.83493s, 100 calls @ 1834932us\n",
      "\n",
      "[11:13:34] LeafPartition: 5.8e-05s, 100 calls @ 58us\n",
      "\n",
      "[11:13:34] UpdatePosition: 1.94801s, 800 calls @ 1948010us\n",
      "\n",
      "[11:13:34] UpdatePredictionCache: 0.149745s, 100 calls @ 149745us\n",
      "\n",
      "[11:13:34] UpdateTree: 10.6044s, 100 calls @ 10604432us\n",
      "\n",
      "[11:13:34] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[11:13:38] ======== Monitor (0): HostSketchContainer ========\n",
      "[11:13:38] AllReduce: 0.016344s, 1 calls @ 16344us\n",
      "\n",
      "[11:13:38] MakeCuts: 0.01754s, 1 calls @ 17540us\n",
      "\n",
      "[11:13:39] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[0]\tvalidation_0-rmse:1.09226\n",
      "[1]\tvalidation_0-rmse:1.09074\n",
      "[2]\tvalidation_0-rmse:1.08846\n",
      "[3]\tvalidation_0-rmse:1.08677\n",
      "[4]\tvalidation_0-rmse:1.08463\n",
      "[5]\tvalidation_0-rmse:1.08313\n",
      "[6]\tvalidation_0-rmse:1.08171\n",
      "[7]\tvalidation_0-rmse:1.07982\n",
      "[8]\tvalidation_0-rmse:1.07887\n",
      "[9]\tvalidation_0-rmse:1.07712\n",
      "[10]\tvalidation_0-rmse:1.07591\n",
      "[11]\tvalidation_0-rmse:1.07469\n",
      "[12]\tvalidation_0-rmse:1.07316\n",
      "[13]\tvalidation_0-rmse:1.07169\n",
      "[14]\tvalidation_0-rmse:1.07054\n",
      "[15]\tvalidation_0-rmse:1.06921\n",
      "[16]\tvalidation_0-rmse:1.06796\n",
      "[17]\tvalidation_0-rmse:1.06710\n",
      "[18]\tvalidation_0-rmse:1.06595\n",
      "[19]\tvalidation_0-rmse:1.06508\n",
      "[20]\tvalidation_0-rmse:1.06403\n",
      "[21]\tvalidation_0-rmse:1.06313\n",
      "[22]\tvalidation_0-rmse:1.06217\n",
      "[23]\tvalidation_0-rmse:1.06146\n",
      "[24]\tvalidation_0-rmse:1.06076\n",
      "[25]\tvalidation_0-rmse:1.06043\n",
      "[26]\tvalidation_0-rmse:1.05998\n",
      "[27]\tvalidation_0-rmse:1.05918\n",
      "[28]\tvalidation_0-rmse:1.05847\n",
      "[29]\tvalidation_0-rmse:1.05776\n",
      "[30]\tvalidation_0-rmse:1.05710\n",
      "[31]\tvalidation_0-rmse:1.05665\n",
      "[32]\tvalidation_0-rmse:1.05619\n",
      "[33]\tvalidation_0-rmse:1.05587\n",
      "[34]\tvalidation_0-rmse:1.05546\n",
      "[35]\tvalidation_0-rmse:1.05490\n",
      "[36]\tvalidation_0-rmse:1.05458\n",
      "[37]\tvalidation_0-rmse:1.05440\n",
      "[38]\tvalidation_0-rmse:1.05403\n",
      "[39]\tvalidation_0-rmse:1.05382\n",
      "[40]\tvalidation_0-rmse:1.05358\n",
      "[41]\tvalidation_0-rmse:1.05311\n",
      "[42]\tvalidation_0-rmse:1.05290\n",
      "[43]\tvalidation_0-rmse:1.05248\n",
      "[44]\tvalidation_0-rmse:1.05219\n",
      "[45]\tvalidation_0-rmse:1.05181\n",
      "[46]\tvalidation_0-rmse:1.05146\n",
      "[47]\tvalidation_0-rmse:1.05111\n",
      "[48]\tvalidation_0-rmse:1.05092\n",
      "[49]\tvalidation_0-rmse:1.05062\n",
      "[50]\tvalidation_0-rmse:1.05039\n",
      "[51]\tvalidation_0-rmse:1.05015\n",
      "[52]\tvalidation_0-rmse:1.04995\n",
      "[53]\tvalidation_0-rmse:1.04976\n",
      "[54]\tvalidation_0-rmse:1.04948\n",
      "[55]\tvalidation_0-rmse:1.04922\n",
      "[56]\tvalidation_0-rmse:1.04910\n",
      "[57]\tvalidation_0-rmse:1.04894\n",
      "[58]\tvalidation_0-rmse:1.04876\n",
      "[59]\tvalidation_0-rmse:1.04862\n",
      "[60]\tvalidation_0-rmse:1.04846\n",
      "[61]\tvalidation_0-rmse:1.04833\n",
      "[62]\tvalidation_0-rmse:1.04817\n",
      "[63]\tvalidation_0-rmse:1.04805\n",
      "[64]\tvalidation_0-rmse:1.04786\n",
      "[65]\tvalidation_0-rmse:1.04779\n",
      "[66]\tvalidation_0-rmse:1.04767\n",
      "[67]\tvalidation_0-rmse:1.04757\n",
      "[68]\tvalidation_0-rmse:1.04749\n",
      "[69]\tvalidation_0-rmse:1.04739\n",
      "[70]\tvalidation_0-rmse:1.04727\n",
      "[71]\tvalidation_0-rmse:1.04713\n",
      "[72]\tvalidation_0-rmse:1.04705\n",
      "[73]\tvalidation_0-rmse:1.04699\n",
      "[74]\tvalidation_0-rmse:1.04690\n",
      "[75]\tvalidation_0-rmse:1.04678\n",
      "[76]\tvalidation_0-rmse:1.04663\n",
      "[77]\tvalidation_0-rmse:1.04656\n",
      "[78]\tvalidation_0-rmse:1.04648\n",
      "[79]\tvalidation_0-rmse:1.04642\n",
      "[80]\tvalidation_0-rmse:1.04630\n",
      "[81]\tvalidation_0-rmse:1.04620\n",
      "[82]\tvalidation_0-rmse:1.04616\n",
      "[83]\tvalidation_0-rmse:1.04609\n",
      "[84]\tvalidation_0-rmse:1.04599\n",
      "[85]\tvalidation_0-rmse:1.04589\n",
      "[86]\tvalidation_0-rmse:1.04581\n",
      "[87]\tvalidation_0-rmse:1.04572\n",
      "[88]\tvalidation_0-rmse:1.04568\n",
      "[89]\tvalidation_0-rmse:1.04562\n",
      "[90]\tvalidation_0-rmse:1.04559\n",
      "[91]\tvalidation_0-rmse:1.04552\n",
      "[92]\tvalidation_0-rmse:1.04546\n",
      "[93]\tvalidation_0-rmse:1.04542\n",
      "[94]\tvalidation_0-rmse:1.04540\n",
      "[95]\tvalidation_0-rmse:1.04537\n",
      "[96]\tvalidation_0-rmse:1.04530\n",
      "[97]\tvalidation_0-rmse:1.04528\n",
      "[98]\tvalidation_0-rmse:1.04525\n",
      "[99]\tvalidation_0-rmse:1.04523\n",
      "[11:13:58] ======== Monitor (0): Learner ========\n",
      "[11:13:58] Configure: 0.000662s, 1 calls @ 662us\n",
      "\n",
      "[11:13:58] EvalOneIter: 7.72255s, 100 calls @ 7722552us\n",
      "\n",
      "[11:13:58] GetGradient: 0.088555s, 100 calls @ 88555us\n",
      "\n",
      "[11:13:58] PredictRaw: 0.000669s, 100 calls @ 669us\n",
      "\n",
      "[11:13:58] UpdateOneIter: 11.0469s, 100 calls @ 11046936us\n",
      "\n",
      "[11:13:58] ======== Monitor (0): GBTree ========\n",
      "[11:13:58] BoostNewTrees: 10.9484s, 100 calls @ 10948395us\n",
      "\n",
      "[11:13:58] CommitModel: 0.00014s, 100 calls @ 140us\n",
      "\n",
      "[11:13:58] ======== Monitor (0): HistUpdater ========\n",
      "[11:13:58] BuildHistogram: 6.32084s, 700 calls @ 6320842us\n",
      "\n",
      "[11:13:58] EvaluateSplits: 0.503983s, 800 calls @ 503983us\n",
      "\n",
      "[11:13:58] InitData: 0.144852s, 100 calls @ 144852us\n",
      "\n",
      "[11:13:58] InitRoot: 1.74723s, 100 calls @ 1747231us\n",
      "\n",
      "[11:13:58] LeafPartition: 3.4e-05s, 100 calls @ 34us\n",
      "\n",
      "[11:13:58] UpdatePosition: 1.95458s, 800 calls @ 1954583us\n",
      "\n",
      "[11:13:58] UpdatePredictionCache: 0.162709s, 100 calls @ 162709us\n",
      "\n",
      "[11:13:58] UpdateTree: 10.6701s, 100 calls @ 10670112us\n",
      "\n",
      "[11:13:58] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[11:14:02] ======== Monitor (0): HostSketchContainer ========\n",
      "[11:14:02] AllReduce: 0.015602s, 1 calls @ 15602us\n",
      "\n",
      "[11:14:02] MakeCuts: 0.015795s, 1 calls @ 15795us\n",
      "\n",
      "[11:14:03] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[0]\tvalidation_0-rmse:1.09040\n",
      "[1]\tvalidation_0-rmse:1.08889\n",
      "[2]\tvalidation_0-rmse:1.08661\n",
      "[3]\tvalidation_0-rmse:1.08495\n",
      "[4]\tvalidation_0-rmse:1.08281\n",
      "[5]\tvalidation_0-rmse:1.08131\n",
      "[6]\tvalidation_0-rmse:1.07991\n",
      "[7]\tvalidation_0-rmse:1.07803\n",
      "[8]\tvalidation_0-rmse:1.07712\n",
      "[9]\tvalidation_0-rmse:1.07538\n",
      "[10]\tvalidation_0-rmse:1.07421\n",
      "[11]\tvalidation_0-rmse:1.07294\n",
      "[12]\tvalidation_0-rmse:1.07140\n",
      "[13]\tvalidation_0-rmse:1.06995\n",
      "[14]\tvalidation_0-rmse:1.06882\n",
      "[15]\tvalidation_0-rmse:1.06750\n",
      "[16]\tvalidation_0-rmse:1.06623\n",
      "[17]\tvalidation_0-rmse:1.06539\n",
      "[18]\tvalidation_0-rmse:1.06424\n",
      "[19]\tvalidation_0-rmse:1.06337\n",
      "[20]\tvalidation_0-rmse:1.06233\n",
      "[21]\tvalidation_0-rmse:1.06143\n",
      "[22]\tvalidation_0-rmse:1.06049\n",
      "[23]\tvalidation_0-rmse:1.05978\n",
      "[24]\tvalidation_0-rmse:1.05910\n",
      "[25]\tvalidation_0-rmse:1.05879\n",
      "[26]\tvalidation_0-rmse:1.05835\n",
      "[27]\tvalidation_0-rmse:1.05756\n",
      "[28]\tvalidation_0-rmse:1.05686\n",
      "[29]\tvalidation_0-rmse:1.05615\n",
      "[30]\tvalidation_0-rmse:1.05550\n",
      "[31]\tvalidation_0-rmse:1.05504\n",
      "[32]\tvalidation_0-rmse:1.05456\n",
      "[33]\tvalidation_0-rmse:1.05424\n",
      "[34]\tvalidation_0-rmse:1.05385\n",
      "[35]\tvalidation_0-rmse:1.05329\n",
      "[36]\tvalidation_0-rmse:1.05297\n",
      "[37]\tvalidation_0-rmse:1.05278\n",
      "[38]\tvalidation_0-rmse:1.05242\n",
      "[39]\tvalidation_0-rmse:1.05222\n",
      "[40]\tvalidation_0-rmse:1.05199\n",
      "[41]\tvalidation_0-rmse:1.05152\n",
      "[42]\tvalidation_0-rmse:1.05133\n",
      "[43]\tvalidation_0-rmse:1.05090\n",
      "[44]\tvalidation_0-rmse:1.05060\n",
      "[45]\tvalidation_0-rmse:1.05023\n",
      "[46]\tvalidation_0-rmse:1.04988\n",
      "[47]\tvalidation_0-rmse:1.04954\n",
      "[48]\tvalidation_0-rmse:1.04936\n",
      "[49]\tvalidation_0-rmse:1.04907\n",
      "[50]\tvalidation_0-rmse:1.04883\n",
      "[51]\tvalidation_0-rmse:1.04862\n",
      "[52]\tvalidation_0-rmse:1.04843\n",
      "[53]\tvalidation_0-rmse:1.04824\n",
      "[54]\tvalidation_0-rmse:1.04797\n",
      "[55]\tvalidation_0-rmse:1.04771\n",
      "[56]\tvalidation_0-rmse:1.04761\n",
      "[57]\tvalidation_0-rmse:1.04747\n",
      "[58]\tvalidation_0-rmse:1.04730\n",
      "[59]\tvalidation_0-rmse:1.04718\n",
      "[60]\tvalidation_0-rmse:1.04703\n",
      "[61]\tvalidation_0-rmse:1.04690\n",
      "[62]\tvalidation_0-rmse:1.04676\n",
      "[63]\tvalidation_0-rmse:1.04665\n",
      "[64]\tvalidation_0-rmse:1.04646\n",
      "[65]\tvalidation_0-rmse:1.04640\n",
      "[66]\tvalidation_0-rmse:1.04630\n",
      "[67]\tvalidation_0-rmse:1.04619\n",
      "[68]\tvalidation_0-rmse:1.04612\n",
      "[69]\tvalidation_0-rmse:1.04603\n",
      "[70]\tvalidation_0-rmse:1.04594\n",
      "[71]\tvalidation_0-rmse:1.04580\n",
      "[72]\tvalidation_0-rmse:1.04574\n",
      "[73]\tvalidation_0-rmse:1.04570\n",
      "[74]\tvalidation_0-rmse:1.04562\n",
      "[75]\tvalidation_0-rmse:1.04548\n",
      "[76]\tvalidation_0-rmse:1.04535\n",
      "[77]\tvalidation_0-rmse:1.04529\n",
      "[78]\tvalidation_0-rmse:1.04523\n",
      "[79]\tvalidation_0-rmse:1.04517\n",
      "[80]\tvalidation_0-rmse:1.04506\n",
      "[81]\tvalidation_0-rmse:1.04496\n",
      "[82]\tvalidation_0-rmse:1.04492\n",
      "[83]\tvalidation_0-rmse:1.04486\n",
      "[84]\tvalidation_0-rmse:1.04477\n",
      "[85]\tvalidation_0-rmse:1.04468\n",
      "[86]\tvalidation_0-rmse:1.04461\n",
      "[87]\tvalidation_0-rmse:1.04452\n",
      "[88]\tvalidation_0-rmse:1.04449\n",
      "[89]\tvalidation_0-rmse:1.04445\n",
      "[90]\tvalidation_0-rmse:1.04443\n",
      "[91]\tvalidation_0-rmse:1.04436\n",
      "[92]\tvalidation_0-rmse:1.04429\n",
      "[93]\tvalidation_0-rmse:1.04425\n",
      "[94]\tvalidation_0-rmse:1.04423\n",
      "[95]\tvalidation_0-rmse:1.04420\n",
      "[96]\tvalidation_0-rmse:1.04414\n",
      "[97]\tvalidation_0-rmse:1.04412\n",
      "[98]\tvalidation_0-rmse:1.04409\n",
      "[99]\tvalidation_0-rmse:1.04408\n",
      "[11:14:22] ======== Monitor (0): Learner ========\n",
      "[11:14:22] Configure: 0.000892s, 1 calls @ 892us\n",
      "\n",
      "[11:14:22] EvalOneIter: 7.65918s, 100 calls @ 7659182us\n",
      "\n",
      "[11:14:22] GetGradient: 0.101555s, 100 calls @ 101555us\n",
      "\n",
      "[11:14:22] PredictRaw: 0.000713s, 100 calls @ 713us\n",
      "\n",
      "[11:14:22] UpdateOneIter: 11.3356s, 100 calls @ 11335627us\n",
      "\n",
      "[11:14:22] ======== Monitor (0): GBTree ========\n",
      "[11:14:22] BoostNewTrees: 11.2136s, 100 calls @ 11213564us\n",
      "\n",
      "[11:14:22] CommitModel: 0.000139s, 100 calls @ 139us\n",
      "\n",
      "[11:14:22] ======== Monitor (0): HistUpdater ========\n",
      "[11:14:22] BuildHistogram: 6.55071s, 700 calls @ 6550712us\n",
      "\n",
      "[11:14:22] EvaluateSplits: 0.50746s, 800 calls @ 507460us\n",
      "\n",
      "[11:14:22] InitData: 0.136785s, 100 calls @ 136785us\n",
      "\n",
      "[11:14:22] InitRoot: 1.79458s, 100 calls @ 1794577us\n",
      "\n",
      "[11:14:22] LeafPartition: 3.7e-05s, 100 calls @ 37us\n",
      "\n",
      "[11:14:22] UpdatePosition: 1.96699s, 800 calls @ 1966986us\n",
      "\n",
      "[11:14:22] UpdatePredictionCache: 0.138163s, 100 calls @ 138163us\n",
      "\n",
      "[11:14:22] UpdateTree: 10.9639s, 100 calls @ 10963859us\n",
      "\n",
      "[11:14:22] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "CPU times: user 7min 12s, sys: 1.52 s, total: 7min 13s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_params = {\n",
    " \"objective\": \"reg:squarederror\",\n",
    " \"eval_metric\": \"rmse\",\n",
    " 'booster': 'gbtree',\n",
    " 'eta': 0.026406022486331556,\n",
    " 'max_depth': 8,\n",
    " 'min_child_weight': 9,\n",
    " 'subsample': 0.9450644777657322,\n",
    " 'colsample_bytree': 0.7330569749023635,\n",
    " 'lambda': 0.3878173140428721,\n",
    " 'alpha': 3.8929025064300062,\n",
    " 'gamma': 0.0934054158792672,\n",
    " \"seed\": SEED,\n",
    " \"verbosity\": 3\n",
    " }\n",
    "\n",
    "# best_params['iterations'] = 500\n",
    "\n",
    "folds_train = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "models_cat = []\n",
    "xgb_OOF = np.zeros(len(x))\n",
    "xgb_preds = np.zeros(len(test))\n",
    "\n",
    "for train_idx, val_idx in folds_train.split(x):\n",
    "    x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
    "    y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        **best_params,\n",
    "        loss_function=\"RMSE\",\n",
    "        random_seed=SEED,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_val, y_val)],\n",
    "        # early_stopping_rounds=100,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    xgb_OOF[val_idx] += model.predict(x_val)\n",
    "    xgb_preds += model.predict(test) / folds_train.n_splits\n",
    "    models_cat.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.0470646013549936\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation RMSE:\", np.sqrt(mean_squared_error(y_log, xgb_OOF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan, test_nan, all_df_nan = prep_nan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan = train_nan.drop('Premium Amount', axis=1)\n",
    "y_nan = train_nan['Premium Amount']\n",
    "\n",
    "y_log_nan = np.log1p(y_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150, step=50), \n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "#         \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "#         \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "#         \"max_features\": trial.suggest_float(\"max_features\", 0.4, 0.9),  \n",
    "#         \"bootstrap\": False,  \n",
    "#         \"random_state\": SEED,\n",
    "#         \"n_jobs\": -1\n",
    "#     }\n",
    "\n",
    "#     scores = []\n",
    "\n",
    "#     # x_sample = x_nan.sample(frac=0.5, random_state=SEED)\n",
    "#     # y_sample = y_log_nan.loc[x_sample.index]\n",
    "#     folds_opt = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "#     # for train_idx, val_idx in folds_opt.split(x_nan):\n",
    "#     for fold, (train_idx, val_idx) in tqdm(enumerate(folds_opt.split(x_nan)), total=folds_opt.get_n_splits()):\n",
    "#         x_train, x_val = x_nan.iloc[train_idx], x_nan.iloc[val_idx]\n",
    "#         y_train, y_val = y_log_nan.iloc[train_idx], y_log_nan.iloc[val_idx]\n",
    "\n",
    "#         model = ExtraTreesRegressor(**params)\n",
    "#         model.fit(x_train, y_train)\n",
    "\n",
    "#         preds = model.predict(x_val)\n",
    "#         score = np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(preds)))\n",
    "#         scores.append(score)\n",
    "#         print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "#     return np.mean(scores)\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=5)\n",
    "\n",
    "# best_params = study.best_params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [44:07<00:00, 529.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 44min 20s, sys: 15.6 s, total: 2h 44min 36s\n",
      "Wall time: 44min 7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 13,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"max_features\": 0.7460208372574245,\n",
    "    \"bootstrap\": False,  \n",
    "    \"random_state\": SEED,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "# best_params['n_estimators'] = 200\n",
    "\n",
    "et_OOF = np.zeros(len(x_nan)) \n",
    "et_preds = np.zeros(len(test_nan)) \n",
    "models_et = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in tqdm(enumerate(folds_train.split(x_nan)), total=folds_train.get_n_splits()):\n",
    "    x_train, x_val = x_nan.iloc[train_idx], x_nan.iloc[val_idx]\n",
    "    y_train, y_val = y_log_nan.iloc[train_idx], y_log_nan.iloc[val_idx]\n",
    "\n",
    "    model = ExtraTreesRegressor(\n",
    "        **best_params,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    et_OOF[val_idx] += model.predict(x_val)\n",
    "    et_preds += model.predict(test_nan) / folds_train.n_splits\n",
    "    models_et.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.0564255745197302\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation RMSE:\", np.sqrt(mean_squared_error(y_log_nan, et_OOF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-30 09:20:41,772] A new study created in memory with name: no-name-5b10aabb-a981-4afe-be28-f95eb6e37e6a\n",
      "[I 2024-12-30 09:20:41,774] Trial 0 finished with value: inf and parameters: {'w1': 0.2037856966824516, 'w2': 0.7586672677831224, 'w3': 0.3344838666132287}. Best is trial 0 with value: inf.\n",
      "[I 2024-12-30 09:20:41,776] Trial 1 finished with value: inf and parameters: {'w1': 0.897955597075071, 'w2': 0.4237314604983533, 'w3': 0.024950591118173437}. Best is trial 0 with value: inf.\n",
      "[I 2024-12-30 09:20:41,794] Trial 2 finished with value: 1.0465886565065712 and parameters: {'w1': 0.21731252233180287, 'w2': 0.16264113226265475, 'w3': 0.5505217662516302}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,795] Trial 3 finished with value: inf and parameters: {'w1': 0.521432195365425, 'w2': 0.8000810201086843, 'w3': 0.9619952172166097}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,796] Trial 4 finished with value: inf and parameters: {'w1': 0.20599361883862577, 'w2': 0.4576155813009346, 'w3': 0.5377672379757985}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,797] Trial 5 finished with value: inf and parameters: {'w1': 0.0012390735110073825, 'w2': 0.8960598792696495, 'w3': 0.34089823543309405}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,798] Trial 6 finished with value: inf and parameters: {'w1': 0.40720843215048497, 'w2': 0.6624562495978636, 'w3': 0.3521070747042928}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,798] Trial 7 finished with value: inf and parameters: {'w1': 0.5740290815608867, 'w2': 0.8816387684640409, 'w3': 0.08393751911968372}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,799] Trial 8 finished with value: inf and parameters: {'w1': 0.6937692924710908, 'w2': 0.5162241855055099, 'w3': 0.637040154021455}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,811] Trial 9 finished with value: 1.0487520153589474 and parameters: {'w1': 0.20154485846128345, 'w2': 0.11734425824753547, 'w3': 0.24204180011909282}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,832] Trial 10 finished with value: 1.047747690309815 and parameters: {'w1': 0.010473727163566093, 'w2': 0.020210005068146297, 'w3': 0.7927379483702723}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,851] Trial 11 finished with value: 1.0477161204187198 and parameters: {'w1': 0.005726814299959304, 'w2': 0.02128225173239473, 'w3': 0.8058479516344863}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,859] Trial 12 finished with value: inf and parameters: {'w1': 0.3207451470684087, 'w2': 0.22761486701693684, 'w3': 0.7324337813128571}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,866] Trial 13 finished with value: inf and parameters: {'w1': 0.10962509530159972, 'w2': 0.26115904927370825, 'w3': 0.9636582929881833}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,873] Trial 14 finished with value: inf and parameters: {'w1': 0.3861653690418968, 'w2': 0.005572520838908246, 'w3': 0.7700111792761075}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,892] Trial 15 finished with value: 1.0468090726449684 and parameters: {'w1': 0.11412115421923394, 'w2': 0.23813042351296237, 'w3': 0.562716392150987}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,900] Trial 16 finished with value: inf and parameters: {'w1': 0.29487945567348256, 'w2': 0.3209503329738304, 'w3': 0.4664657489671925}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,908] Trial 17 finished with value: inf and parameters: {'w1': 0.6397964006973293, 'w2': 0.3304906945137027, 'w3': 0.5709296449508994}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,933] Trial 18 finished with value: 1.0474915865995955 and parameters: {'w1': 0.15247535257313227, 'w2': 0.16179929237338628, 'w3': 0.44228154917934537}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,945] Trial 19 finished with value: inf and parameters: {'w1': 0.8195015272625878, 'w2': 0.5651362512206592, 'w3': 0.6565034117086406}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,958] Trial 20 finished with value: inf and parameters: {'w1': 0.4251140254287167, 'w2': 0.37730362385541655, 'w3': 0.6620842463537919}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:41,984] Trial 21 finished with value: 1.0477038956389397 and parameters: {'w1': 0.1238875760022878, 'w2': 0.16240503969172967, 'w3': 0.44615532884791687}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:42,009] Trial 22 finished with value: 1.0499425919345309 and parameters: {'w1': 0.11126929177736394, 'w2': 0.14198579152155866, 'w3': 0.20143435490166728}. Best is trial 2 with value: 1.0465886565065712.\n",
      "[I 2024-12-30 09:20:42,033] Trial 23 finished with value: 1.0462581655820304 and parameters: {'w1': 0.31638819198279705, 'w2': 0.22947578728573065, 'w3': 0.42991319393629585}. Best is trial 23 with value: 1.0462581655820304.\n",
      "[I 2024-12-30 09:20:42,044] Trial 24 finished with value: inf and parameters: {'w1': 0.309432491166288, 'w2': 0.25681282519126214, 'w3': 0.5665899232181651}. Best is trial 23 with value: 1.0462581655820304.\n",
      "[I 2024-12-30 09:20:42,068] Trial 25 finished with value: 1.0473769558330386 and parameters: {'w1': 0.2587995280666628, 'w2': 0.09170432834899675, 'w3': 0.39330484743611727}. Best is trial 23 with value: 1.0462581655820304.\n",
      "[I 2024-12-30 09:20:42,092] Trial 26 finished with value: 1.0460738629428554 and parameters: {'w1': 0.48383600054398657, 'w2': 0.2251939007853679, 'w3': 0.2663853350181016}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,104] Trial 27 finished with value: inf and parameters: {'w1': 0.4364958413068967, 'w2': 0.5750666209859703, 'w3': 0.23737625283456837}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,115] Trial 28 finished with value: inf and parameters: {'w1': 0.4974961312792575, 'w2': 0.3391599502167883, 'w3': 0.1649025483399782}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,123] Trial 29 finished with value: inf and parameters: {'w1': 0.7361822340398845, 'w2': 0.20279897673659342, 'w3': 0.29486016487732697}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,132] Trial 30 finished with value: inf and parameters: {'w1': 0.3558855567348781, 'w2': 0.9895490514432695, 'w3': 0.1444566746073671}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,140] Trial 31 finished with value: inf and parameters: {'w1': 0.23497903044443866, 'w2': 0.2836912907891143, 'w3': 0.5067369572874842}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,161] Trial 32 finished with value: 1.049196097526718 and parameters: {'w1': 0.0787801915127852, 'w2': 0.07696667924372529, 'w3': 0.4011037663952084}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,170] Trial 33 finished with value: inf and parameters: {'w1': 0.48797414551079665, 'w2': 0.42472929780547775, 'w3': 0.302980327513137}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,178] Trial 34 finished with value: inf and parameters: {'w1': 0.9865972058839552, 'w2': 0.18657538664581427, 'w3': 0.6265216750012647}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,185] Trial 35 finished with value: inf and parameters: {'w1': 0.173130251995199, 'w2': 0.3841161039135331, 'w3': 0.5028475011160702}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,193] Trial 36 finished with value: inf and parameters: {'w1': 0.25540715930161817, 'w2': 0.0719920419372665, 'w3': 0.8568940350452814}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,202] Trial 37 finished with value: inf and parameters: {'w1': 0.5887616076853025, 'w2': 0.48116082141942024, 'w3': 0.06082750475895926}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,222] Trial 38 finished with value: 1.0462953637707992 and parameters: {'w1': 0.34037274526877714, 'w2': 0.22661247648550964, 'w3': 0.3810759435991004}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,231] Trial 39 finished with value: inf and parameters: {'w1': 0.5477772762916686, 'w2': 0.30686686607908376, 'w3': 0.3779763985375039}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,239] Trial 40 finished with value: inf and parameters: {'w1': 0.4584083249825227, 'w2': 0.39575758735921773, 'w3': 0.30202608191044583}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,247] Trial 41 finished with value: inf and parameters: {'w1': 0.36287344373896435, 'w2': 0.22548253719409295, 'w3': 0.5871032183704927}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,267] Trial 42 finished with value: 1.0480588137898088 and parameters: {'w1': 0.05250965470493833, 'w2': 0.1337500446539912, 'w3': 0.5262992723148445}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,287] Trial 43 finished with value: 1.046808350514778 and parameters: {'w1': 0.1979910727487886, 'w2': 0.2273931965015773, 'w3': 0.44262814217466245}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,306] Trial 44 finished with value: 1.0473162917641956 and parameters: {'w1': 0.2227554269740002, 'w2': 0.19284161013317658, 'w3': 0.338738784645456}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,315] Trial 45 finished with value: inf and parameters: {'w1': 0.28316539456180834, 'w2': 0.7144504171055686, 'w3': 0.4285352815346314}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,336] Trial 46 finished with value: 1.046671356959751 and parameters: {'w1': 0.3425825671617311, 'w2': 0.04493561180272726, 'w3': 0.48156747716271187}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,356] Trial 47 finished with value: 1.046589377469993 and parameters: {'w1': 0.3511098622630918, 'w2': 0.05504218175093581, 'w3': 0.47820675498863885}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,376] Trial 48 finished with value: 1.0468142173823942 and parameters: {'w1': 0.41826414230100994, 'w2': 0.11287000253377188, 'w3': 0.2602833336347829}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,395] Trial 49 finished with value: 1.0469333412847297 and parameters: {'w1': 0.369557862956871, 'w2': 0.059859229579912326, 'w3': 0.36292015873176336}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,404] Trial 50 finished with value: inf and parameters: {'w1': 0.6309650608050739, 'w2': 0.005991436786651538, 'w3': 0.7301414348028753}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,423] Trial 51 finished with value: 1.0466954178335461 and parameters: {'w1': 0.3399445306262914, 'w2': 0.04798912091978107, 'w3': 0.4751797391811837}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,432] Trial 52 finished with value: inf and parameters: {'w1': 0.3183898169094996, 'w2': 0.10972492818464738, 'w3': 0.6064634445836773}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,453] Trial 53 finished with value: 1.0469212238519847 and parameters: {'w1': 0.2683453880376139, 'w2': 0.04402216219451116, 'w3': 0.5314091959882666}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,462] Trial 54 finished with value: inf and parameters: {'w1': 0.39369754469896034, 'w2': 0.16142861736479105, 'w3': 0.4827155710475085}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,471] Trial 55 finished with value: inf and parameters: {'w1': 0.46164243215901946, 'w2': 0.2615325471735573, 'w3': 0.41486390341567725}. Best is trial 26 with value: 1.0460738629428554.\n",
      "[I 2024-12-30 09:20:42,492] Trial 56 finished with value: 1.0460386424519361 and parameters: {'w1': 0.5221597609329593, 'w2': 0.12932788582042773, 'w3': 0.3351079445336575}. Best is trial 56 with value: 1.0460386424519361.\n",
      "[I 2024-12-30 09:20:42,501] Trial 57 finished with value: inf and parameters: {'w1': 0.5551666845171254, 'w2': 0.2881843536546006, 'w3': 0.2668623374853009}. Best is trial 56 with value: 1.0460386424519361.\n",
      "[I 2024-12-30 09:20:42,523] Trial 58 finished with value: 1.0462434192275607 and parameters: {'w1': 0.5093610455787693, 'w2': 0.18178335293568637, 'w3': 0.21205039911729684}. Best is trial 56 with value: 1.0460386424519361.\n",
      "[I 2024-12-30 09:20:42,543] Trial 59 finished with value: 1.046255669312813 and parameters: {'w1': 0.5298576858760071, 'w2': 0.19188893217309644, 'w3': 0.17186982268277345}. Best is trial 56 with value: 1.0460386424519361.\n",
      "[I 2024-12-30 09:20:42,552] Trial 60 finished with value: inf and parameters: {'w1': 0.6258280712889156, 'w2': 0.3587795169950191, 'w3': 0.15991102851452063}. Best is trial 56 with value: 1.0460386424519361.\n",
      "[I 2024-12-30 09:20:42,573] Trial 61 finished with value: 1.0461779419839687 and parameters: {'w1': 0.5229631326178522, 'w2': 0.1884285827890013, 'w3': 0.21000119577957638}. Best is trial 56 with value: 1.0460386424519361.\n",
      "[I 2024-12-30 09:20:42,592] Trial 62 finished with value: 1.0459887320949202 and parameters: {'w1': 0.6829547328023751, 'w2': 0.1874333115104145, 'w3': 0.10256262568766673}. Best is trial 62 with value: 1.0459887320949202.\n",
      "[I 2024-12-30 09:20:42,610] Trial 63 finished with value: 1.0461136754582612 and parameters: {'w1': 0.7373690762050796, 'w2': 0.17629632002582654, 'w3': 0.0017282476839604322}. Best is trial 62 with value: 1.0459887320949202.\n",
      "[I 2024-12-30 09:20:42,630] Trial 64 finished with value: 1.0460650688856974 and parameters: {'w1': 0.7276614959855413, 'w2': 0.183121520465384, 'w3': 0.024988555679139546}. Best is trial 62 with value: 1.0459887320949202.\n",
      "[I 2024-12-30 09:20:42,649] Trial 65 finished with value: 1.0461183450672975 and parameters: {'w1': 0.7475619263667277, 'w2': 0.14965673259898954, 'w3': 0.014265913874958777}. Best is trial 62 with value: 1.0459887320949202.\n",
      "[I 2024-12-30 09:20:42,659] Trial 66 finished with value: inf and parameters: {'w1': 0.773450345468057, 'w2': 0.13719698231761285, 'w3': 0.10660741083731423}. Best is trial 62 with value: 1.0459887320949202.\n",
      "[I 2024-12-30 09:20:42,679] Trial 67 finished with value: 1.046003940641145 and parameters: {'w1': 0.848161185531847, 'w2': 0.11155494265721107, 'w3': 0.013256897990019435}. Best is trial 62 with value: 1.0459887320949202.\n",
      "[I 2024-12-30 09:20:42,698] Trial 68 finished with value: 1.0459987204573202 and parameters: {'w1': 0.8543450977171269, 'w2': 0.11627253477110171, 'w3': 0.007873857064415082}. Best is trial 62 with value: 1.0459887320949202.\n",
      "[I 2024-12-30 09:20:42,719] Trial 69 finished with value: 1.0459782966791717 and parameters: {'w1': 0.8418197076926294, 'w2': 0.09519922121859947, 'w3': 0.049848592365118696}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,729] Trial 70 finished with value: inf and parameters: {'w1': 0.8698912605103108, 'w2': 0.10860044894512351, 'w3': 0.05085943032001157}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,750] Trial 71 finished with value: 1.0460773643922052 and parameters: {'w1': 0.8385781142122857, 'w2': 0.0924079375410593, 'w3': 0.0009581875835403039}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,760] Trial 72 finished with value: inf and parameters: {'w1': 0.9191689836837542, 'w2': 0.09059490830299927, 'w3': 0.09823402391569501}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,781] Trial 73 finished with value: 1.04614298583249 and parameters: {'w1': 0.8248604296115555, 'w2': 0.02499296870585703, 'w3': 0.05724069328185188}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,802] Trial 74 finished with value: 1.0464903902639786 and parameters: {'w1': 0.6803362575488654, 'w2': 0.09330808220671598, 'w3': 0.03326059137736358}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,812] Trial 75 finished with value: inf and parameters: {'w1': 0.918452170596348, 'w2': 0.12307973352215257, 'w3': 0.13058224891633072}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,820] Trial 76 finished with value: inf and parameters: {'w1': 0.8250502286045488, 'w2': 0.14740175769720887, 'w3': 0.0793352187513904}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,828] Trial 77 finished with value: inf and parameters: {'w1': 0.7831937715736522, 'w2': 0.25793229770822695, 'w3': 0.035385067135018995}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,836] Trial 78 finished with value: inf and parameters: {'w1': 0.866880963169785, 'w2': 0.2090197052263441, 'w3': 0.12175876587550713}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,845] Trial 79 finished with value: inf and parameters: {'w1': 0.9689238960405745, 'w2': 0.6104155061398242, 'w3': 0.0786748712393015}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,853] Trial 80 finished with value: inf and parameters: {'w1': 0.7077912057871831, 'w2': 0.8408311469831435, 'w3': 0.0016171834690132808}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,862] Trial 81 finished with value: inf and parameters: {'w1': 0.8526344047111298, 'w2': 0.16672063205499663, 'w3': 0.007357601109424733}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,882] Trial 82 finished with value: 1.0461305331074002 and parameters: {'w1': 0.7888153221239713, 'w2': 0.07852913180919058, 'w3': 0.039312515207261124}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,901] Trial 83 finished with value: 1.0462447622844913 and parameters: {'w1': 0.6719772840310007, 'w2': 0.12899280613322242, 'w3': 0.07178830411754872}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,920] Trial 84 finished with value: 1.0460629489762514 and parameters: {'w1': 0.7360900297912067, 'w2': 0.09732501041476047, 'w3': 0.10024801406574799}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,930] Trial 85 finished with value: inf and parameters: {'w1': 0.8935405696375298, 'w2': 0.029956398850697674, 'w3': 0.09940862086069391}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,951] Trial 86 finished with value: 1.0460848847827202 and parameters: {'w1': 0.7136145717658126, 'w2': 0.08352049452051702, 'w3': 0.12987354616115407}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,960] Trial 87 finished with value: inf and parameters: {'w1': 0.8072689768627449, 'w2': 0.10383983225740825, 'w3': 0.1868254548069461}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,980] Trial 88 finished with value: 1.0475342530010918 and parameters: {'w1': 0.6046697315086766, 'w2': 0.0017002692387728668, 'w3': 0.027100532987912908}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,990] Trial 89 finished with value: inf and parameters: {'w1': 0.7626474501473306, 'w2': 0.20836251056959412, 'w3': 0.05943043800170937}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:42,998] Trial 90 finished with value: inf and parameters: {'w1': 0.940736891690588, 'w2': 0.06196453041733152, 'w3': 0.1472726070003916}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,019] Trial 91 finished with value: 1.0460723229986195 and parameters: {'w1': 0.7194824154569388, 'w2': 0.08544278233792632, 'w3': 0.12661850437221536}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,039] Trial 92 finished with value: 1.0461777475406617 and parameters: {'w1': 0.6688064283982234, 'w2': 0.13232931229595032, 'w3': 0.09343651869406691}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,048] Trial 93 finished with value: inf and parameters: {'w1': 0.8427290234422836, 'w2': 0.1663102508687485, 'w3': 0.12166834507935545}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,056] Trial 94 finished with value: inf and parameters: {'w1': 0.8039851840843325, 'w2': 0.0707595474348512, 'w3': 0.2331253076797114}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,076] Trial 95 finished with value: 1.0469819982268693 and parameters: {'w1': 0.6531621546102907, 'w2': 0.03130240638471257, 'w3': 0.0289495558411189}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,085] Trial 96 finished with value: inf and parameters: {'w1': 0.8898385835827284, 'w2': 0.11318216291346139, 'w3': 0.06489240901636434}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,105] Trial 97 finished with value: 1.0461331153690923 and parameters: {'w1': 0.714606568368006, 'w2': 0.14834859416893748, 'w3': 0.043576558640548324}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,114] Trial 98 finished with value: inf and parameters: {'w1': 0.7519779419710898, 'w2': 0.24487863825454131, 'w3': 0.17810189677008043}. Best is trial 69 with value: 1.0459782966791717.\n",
      "[I 2024-12-30 09:20:43,123] Trial 99 finished with value: inf and parameters: {'w1': 0.8429073632845279, 'w2': 0.08957589764751238, 'w3': 0.0840896179906684}. Best is trial 69 with value: 1.0459782966791717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Weights: {'w1': 0.8418197076926294, 'w2': 0.09519922121859947, 'w3': 0.049848592365118696}\n",
      "Best RMSE: 1.0460\n",
      "CPU times: user 1.31 s, sys: 55.3 ms, total: 1.37 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    w1 = trial.suggest_float('w1', 0.0, 1.0)\n",
    "    w2 = trial.suggest_float('w2', 0.0, 1.0)\n",
    "    w3 = trial.suggest_float('w3', 0.0, 1.0)\n",
    "    w4 = 1.0 - (w1 + w2 + w3)\n",
    "\n",
    "    if w4 < 0 or w4 > 1:\n",
    "        return float('inf')\n",
    "    \n",
    "    if w3 < 0 or w3 > 1:\n",
    "        return float('inf')\n",
    "    \n",
    "    ensemble_vote = (w1 * lgbm_OOF) + (w2 * cat_OOF) + (w3 * xgb_OOF) + (w4 * et_OOF)\n",
    "    rmse = np.sqrt(mean_squared_error(y_log, ensemble_vote))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "study_vote = optuna.create_study(direction='minimize')\n",
    "study_vote.optimize(objective, n_trials=100)\n",
    "\n",
    "#    RMSE \n",
    "print(f\"Best Weights: {study_vote.best_params}\")\n",
    "print(f\"Best RMSE: {study_vote.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = study_vote.best_params\n",
    "best_weights['w4'] = 1 - best_weights['w1'] - best_weights['w2'] - best_weights['w3']\n",
    "preds_exp = (best_weights['w1'] * lgbm_preds) + (best_weights['w2'] * cat_preds) + (best_weights['w3'] * xgb_preds) + (best_weights['w4'] * et_preds)\n",
    "preds = np.expm1(preds_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOF_std = np.std([lgbm_OOF, xgb_OOF, cat_OOF, et_OOF], axis=0)\n",
    "pred_std = np.std([lgbm_preds, xgb_preds, cat_preds, et_preds], axis=0)\n",
    "\n",
    "OOF_mean = np.mean([lgbm_OOF, xgb_OOF, cat_OOF, et_OOF], axis=0)\n",
    "OOF_min = np.min([lgbm_OOF, xgb_OOF, cat_OOF, et_OOF], axis=0)\n",
    "OOF_max = np.max([lgbm_OOF, xgb_OOF, cat_OOF, et_OOF], axis=0)\n",
    "\n",
    "pred_mean = np.mean([lgbm_preds, xgb_preds, cat_preds, et_preds], axis=0)\n",
    "pred_min = np.min([lgbm_preds, xgb_preds, cat_preds, et_preds], axis=0)\n",
    "pred_max = np.max([lgbm_preds, xgb_preds, cat_preds, et_preds], axis=0)\n",
    "\n",
    "stacked_train = np.column_stack((lgbm_OOF, xgb_OOF, cat_OOF, et_OOF, train_nan['transformed_Annual_Income'], train_nan['Credit Score'], OOF_std, OOF_mean, OOF_min, OOF_max))\n",
    "stacked_test = np.column_stack((lgbm_preds, xgb_preds, cat_preds, et_preds, test_nan['transformed_Annual_Income'], test_nan['Credit Score'], pred_std, pred_mean, pred_min, pred_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-30 12:26:50,227] A new study created in memory with name: no-name-7d6bbfa1-1878-409f-8dc0-8e3046cf4ad1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-30 12:26:51,019] Trial 0 finished with value: -1.045705408684945 and parameters: {'alpha': 0.0059625243249216645}. Best is trial 0 with value: -1.045705408684945.\n",
      "[I 2024-12-30 12:26:51,733] Trial 1 finished with value: -1.0457054221736843 and parameters: {'alpha': 1.1594567652674188}. Best is trial 0 with value: -1.045705408684945.\n",
      "[I 2024-12-30 12:26:52,461] Trial 2 finished with value: -1.0457054065333868 and parameters: {'alpha': 0.0445799810134213}. Best is trial 2 with value: -1.0457054065333868.\n",
      "[I 2024-12-30 12:26:53,175] Trial 3 finished with value: -1.045705408976231 and parameters: {'alpha': 0.0010317987837480335}. Best is trial 2 with value: -1.0457054065333868.\n",
      "[I 2024-12-30 12:26:53,959] Trial 4 finished with value: -1.0457054084884927 and parameters: {'alpha': 0.009324676967112495}. Best is trial 2 with value: -1.0457054065333868.\n",
      "[I 2024-12-30 12:26:54,701] Trial 5 finished with value: -1.0457054045965024 and parameters: {'alpha': 0.08343844740358762}. Best is trial 5 with value: -1.0457054045965024.\n",
      "[I 2024-12-30 12:26:55,478] Trial 6 finished with value: -1.045705408739704 and parameters: {'alpha': 0.005030682926772205}. Best is trial 5 with value: -1.0457054045965024.\n",
      "[I 2024-12-30 12:26:56,265] Trial 7 finished with value: -1.0457054082670987 and parameters: {'alpha': 0.013150202656765484}. Best is trial 5 with value: -1.0457054045965024.\n",
      "[I 2024-12-30 12:26:57,023] Trial 8 finished with value: -1.045705398335103 and parameters: {'alpha': 0.2684352229059763}. Best is trial 8 with value: -1.045705398335103.\n",
      "[I 2024-12-30 12:26:57,703] Trial 9 finished with value: -1.0457054072174998 and parameters: {'alpha': 0.892037438092577}. Best is trial 8 with value: -1.045705398335103.\n",
      "[I 2024-12-30 12:26:58,371] Trial 10 finished with value: -1.0457053990922276 and parameters: {'alpha': 0.2360757428933729}. Best is trial 8 with value: -1.045705398335103.\n",
      "[I 2024-12-30 12:26:59,119] Trial 11 finished with value: -1.0457053983904783 and parameters: {'alpha': 0.2658525715133403}. Best is trial 8 with value: -1.045705398335103.\n",
      "[I 2024-12-30 12:26:59,885] Trial 12 finished with value: -1.0457053985482967 and parameters: {'alpha': 0.25870411554725625}. Best is trial 8 with value: -1.045705398335103.\n",
      "[I 2024-12-30 12:27:00,621] Trial 13 finished with value: -1.0457053974336863 and parameters: {'alpha': 0.3183617461302181}. Best is trial 13 with value: -1.0457053974336863.\n",
      "[I 2024-12-30 12:27:01,338] Trial 14 finished with value: -1.045705405262275 and parameters: {'alpha': 0.06956202881202245}. Best is trial 13 with value: -1.0457053974336863.\n",
      "[I 2024-12-30 12:27:02,081] Trial 15 finished with value: -1.0457054922184674 and parameters: {'alpha': 1.9411445446541424}. Best is trial 13 with value: -1.0457053974336863.\n",
      "[I 2024-12-30 12:27:02,912] Trial 16 finished with value: -1.0457053967787826 and parameters: {'alpha': 0.5069512772631732}. Best is trial 16 with value: -1.0457053967787826.\n",
      "[I 2024-12-30 12:27:03,628] Trial 17 finished with value: -1.0457053999864667 and parameters: {'alpha': 0.6921074800812175}. Best is trial 16 with value: -1.0457053967787826.\n",
      "[I 2024-12-30 12:27:04,376] Trial 18 finished with value: -1.045705402528545 and parameters: {'alpha': 0.1310019467846817}. Best is trial 16 with value: -1.0457053967787826.\n",
      "[I 2024-12-30 12:27:05,121] Trial 19 finished with value: -1.0457054074978767 and parameters: {'alpha': 0.026758310372896384}. Best is trial 16 with value: -1.0457053967787826.\n",
      "[I 2024-12-30 12:27:05,824] Trial 20 finished with value: -1.0457053971911088 and parameters: {'alpha': 0.5491277976792498}. Best is trial 16 with value: -1.0457053967787826.\n",
      "[I 2024-12-30 12:27:06,581] Trial 21 finished with value: -1.0457053972208974 and parameters: {'alpha': 0.5515682650425912}. Best is trial 16 with value: -1.0457053967787826.\n",
      "[I 2024-12-30 12:27:07,398] Trial 22 finished with value: -1.0457053971760732 and parameters: {'alpha': 0.5478755417315381}. Best is trial 16 with value: -1.0457053967787826.\n",
      "[I 2024-12-30 12:27:08,126] Trial 23 finished with value: -1.0457054958089893 and parameters: {'alpha': 1.9738192693870618}. Best is trial 16 with value: -1.0457053967787826.\n",
      "[I 2024-12-30 12:27:08,863] Trial 24 finished with value: -1.04570539657298 and parameters: {'alpha': 0.46788079150732664}. Best is trial 24 with value: -1.04570539657298.\n",
      "[I 2024-12-30 12:27:09,677] Trial 25 finished with value: -1.045705402251982 and parameters: {'alpha': 0.13799609959865872}. Best is trial 24 with value: -1.04570539657298.\n",
      "[I 2024-12-30 12:27:10,418] Trial 26 finished with value: -1.045705396545532 and parameters: {'alpha': 0.42613658498249335}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:11,217] Trial 27 finished with value: -1.0457054191203166 and parameters: {'alpha': 1.1120268149876964}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:11,984] Trial 28 finished with value: -1.0457054030599926 and parameters: {'alpha': 0.11803068269483764}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:12,668] Trial 29 finished with value: -1.0457053967512688 and parameters: {'alpha': 0.38025977243991066}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:13,372] Trial 30 finished with value: -1.045705406793752 and parameters: {'alpha': 0.03967990887482828}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:14,072] Trial 31 finished with value: -1.0457053965596166 and parameters: {'alpha': 0.4195901799858837}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:14,854] Trial 32 finished with value: -1.0457054251954094 and parameters: {'alpha': 1.2041114712540304}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:15,593] Trial 33 finished with value: -1.0457053966862255 and parameters: {'alpha': 0.390056964442569}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:16,327] Trial 34 finished with value: -1.0457054009116136 and parameters: {'alpha': 0.1747703401875441}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:17,089] Trial 35 finished with value: -1.0457054062631872 and parameters: {'alpha': 0.8704480661206906}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:17,763] Trial 36 finished with value: -1.0457054048967018 and parameters: {'alpha': 0.07710768969282834}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:18,486] Trial 37 finished with value: -1.045705396644165 and parameters: {'alpha': 0.3975694128360232}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:19,232] Trial 38 finished with value: -1.0457054089753106 and parameters: {'alpha': 0.0010472773247086399}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:20,027] Trial 39 finished with value: -1.0457054005043933 and parameters: {'alpha': 0.18710240487445057}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:20,785] Trial 40 finished with value: -1.0457054313140812 and parameters: {'alpha': 1.2889562153064427}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:21,553] Trial 41 finished with value: -1.0457053969231334 and parameters: {'alpha': 0.359953020744945}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:22,281] Trial 42 finished with value: -1.0457054011503124 and parameters: {'alpha': 0.7325784516813284}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:22,997] Trial 43 finished with value: -1.0457053966283858 and parameters: {'alpha': 0.400751317620628}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:23,765] Trial 44 finished with value: -1.0457054003498458 and parameters: {'alpha': 0.19195687112272652}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:24,563] Trial 45 finished with value: -1.0457054037383828 and parameters: {'alpha': 0.10227040962178478}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:25,308] Trial 46 finished with value: -1.0457053967732644 and parameters: {'alpha': 0.377291275474235}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:26,094] Trial 47 finished with value: -1.0457054087847435 and parameters: {'alpha': 0.0042659589322029635}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:26,841] Trial 48 finished with value: -1.0457054383141853 and parameters: {'alpha': 1.3789019060670362}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:27,538] Trial 49 finished with value: -1.0457054056570805 and parameters: {'alpha': 0.06160278151154624}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:28,271] Trial 50 finished with value: -1.0457053986536884 and parameters: {'alpha': 0.25409145689476725}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:28,983] Trial 51 finished with value: -1.0457053966181846 and parameters: {'alpha': 0.4029507765943976}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:29,710] Trial 52 finished with value: -1.0457054014206513 and parameters: {'alpha': 0.7412654867503189}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:30,457] Trial 53 finished with value: -1.0457053965774021 and parameters: {'alpha': 0.4692892682060638}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:31,197] Trial 54 finished with value: -1.045705396677104 and parameters: {'alpha': 0.4915392616854482}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:31,916] Trial 55 finished with value: -1.045705398341462 and parameters: {'alpha': 0.26813657314042644}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:32,600] Trial 56 finished with value: -1.0457054073741348 and parameters: {'alpha': 0.8954966543976387}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:33,309] Trial 57 finished with value: -1.0457054003871504 and parameters: {'alpha': 0.190775746970756}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:34,012] Trial 58 finished with value: -1.0457053974201078 and parameters: {'alpha': 0.5667159204587328}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:34,728] Trial 59 finished with value: -1.0457054079940988 and parameters: {'alpha': 0.017922282940415757}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:35,466] Trial 60 finished with value: -1.045705456632439 and parameters: {'alpha': 1.5898763696432705}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:36,270] Trial 61 finished with value: -1.04570539740869 and parameters: {'alpha': 0.32005458353129784}. Best is trial 26 with value: -1.045705396545532.\n",
      "[I 2024-12-30 12:27:37,033] Trial 62 finished with value: -1.0457053965392853 and parameters: {'alpha': 0.4302260719005836}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:37,721] Trial 63 finished with value: -1.0457053965439325 and parameters: {'alpha': 0.45554291126920793}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:38,509] Trial 64 finished with value: -1.0457053980637045 and parameters: {'alpha': 0.6067448960218532}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:39,254] Trial 65 finished with value: -1.0457053965791177 and parameters: {'alpha': 0.4698170712076721}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:39,973] Trial 66 finished with value: -1.0457054017235674 and parameters: {'alpha': 0.15188025588188245}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:40,753] Trial 67 finished with value: -1.0457054137478188 and parameters: {'alpha': 1.0213171546404407}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:41,559] Trial 68 finished with value: -1.0457053995480372 and parameters: {'alpha': 0.21899116081397885}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:42,271] Trial 69 finished with value: -1.0457054015724645 and parameters: {'alpha': 0.7460456412103564}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:43,009] Trial 70 finished with value: -1.0457054036892728 and parameters: {'alpha': 0.10338382007850373}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:43,828] Trial 71 finished with value: -1.0457053966024237 and parameters: {'alpha': 0.4762131302791067}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:44,570] Trial 72 finished with value: -1.0457053965777081 and parameters: {'alpha': 0.4693841573252693}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:45,365] Trial 73 finished with value: -1.0457053972913475 and parameters: {'alpha': 0.328354476291407}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:46,180] Trial 74 finished with value: -1.045705409907872 and parameters: {'alpha': 0.9486447156224278}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:46,970] Trial 75 finished with value: -1.045705397588982 and parameters: {'alpha': 0.30834611560469566}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:47,670] Trial 76 finished with value: -1.0457053965821472 and parameters: {'alpha': 0.4707263439599419}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:48,429] Trial 77 finished with value: -1.0457053980933158 and parameters: {'alpha': 0.6083665520493797}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:49,153] Trial 78 finished with value: -1.045705399095685 and parameters: {'alpha': 0.23594040071602637}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:49,946] Trial 79 finished with value: -1.0457053986456402 and parameters: {'alpha': 0.6362855442138213}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:50,672] Trial 80 finished with value: -1.0457054061763604 and parameters: {'alpha': 0.8684373180824868}. Best is trial 62 with value: -1.0457053965392853.\n",
      "[I 2024-12-30 12:27:51,422] Trial 81 finished with value: -1.0457053965353742 and parameters: {'alpha': 0.44871934026195315}. Best is trial 81 with value: -1.0457053965353742.\n",
      "[I 2024-12-30 12:27:52,126] Trial 82 finished with value: -1.0457053965657426 and parameters: {'alpha': 0.46540070561192015}. Best is trial 81 with value: -1.0457053965353742.\n",
      "[I 2024-12-30 12:27:52,859] Trial 83 finished with value: -1.045705398277957 and parameters: {'alpha': 0.27114401337518806}. Best is trial 81 with value: -1.0457053965353742.\n",
      "[I 2024-12-30 12:27:53,627] Trial 84 finished with value: -1.045705400530192 and parameters: {'alpha': 0.7117027930356422}. Best is trial 81 with value: -1.0457053965353742.\n",
      "[I 2024-12-30 12:27:54,336] Trial 85 finished with value: -1.0457053965327976 and parameters: {'alpha': 0.43800046250992997}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:27:55,046] Trial 86 finished with value: -1.045705397484538 and parameters: {'alpha': 0.3149908112992667}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:27:55,811] Trial 87 finished with value: -1.0457054014251999 and parameters: {'alpha': 0.16005426828294864}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:27:56,576] Trial 88 finished with value: -1.0457054477132168 and parameters: {'alpha': 1.4907955359673901}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:27:57,395] Trial 89 finished with value: -1.0457053965371965 and parameters: {'alpha': 0.43198822744237897}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:27:58,175] Trial 90 finished with value: -1.045705420355564 and parameters: {'alpha': 1.131522194338609}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:27:58,965] Trial 91 finished with value: -1.0457053965823664 and parameters: {'alpha': 0.4119708355034491}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:27:59,713] Trial 92 finished with value: -1.0457053978215205 and parameters: {'alpha': 0.2946750719243164}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:28:00,509] Trial 93 finished with value: -1.0457053996013008 and parameters: {'alpha': 0.21708581365455484}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:28:01,350] Trial 94 finished with value: -1.0457054039322444 and parameters: {'alpha': 0.8132467517873908}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:28:02,101] Trial 95 finished with value: -1.045705396894356 and parameters: {'alpha': 0.5209887970182947}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:28:02,840] Trial 96 finished with value: -1.0457053969180368 and parameters: {'alpha': 0.36048004129526556}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:28:03,563] Trial 97 finished with value: -1.0457054086604345 and parameters: {'alpha': 0.006380376017753591}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:28:04,317] Trial 98 finished with value: -1.0457053981639768 and parameters: {'alpha': 0.6121775292849009}. Best is trial 85 with value: -1.0457053965327976.\n",
      "[I 2024-12-30 12:28:05,035] Trial 99 finished with value: -1.0457053965546472 and parameters: {'alpha': 0.42164760380840394}. Best is trial 85 with value: -1.0457053965327976.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-3, 2)\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    \n",
    "    score = cross_val_score(ridge, stacked_train, y_log_nan, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    return score.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.43800046250992997\n",
      "Best score: -1.0457053965327976\n"
     ]
    }
   ],
   "source": [
    "best_alpha = study.best_params['alpha']\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best score: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. -1.0457168927031868\n",
    "2. -1.045713173575535\n",
    "3. -1.045716496317705\n",
    "4. -1.0457053965327976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.66749907 6.6732041  6.67855243 6.69526801 6.63341814 6.66475219\n",
      " 6.87281544 6.61364164 5.37320411 6.71519926]\n"
     ]
    }
   ],
   "source": [
    "meta_model = Ridge(alpha=best_alpha)\n",
    "meta_model.fit(stacked_train, y_log_nan)\n",
    "\n",
    "stacked_preds = meta_model.predict(stacked_test)\n",
    "print(stacked_preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.expm1(stacked_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('./data/pred_oof_data.npz',\n",
    "         lgbm_OOF=lgbm_OOF,\n",
    "         xgb_OOF=xgb_OOF,\n",
    "         cat_OOF=cat_OOF,\n",
    "         et_OOF=et_OOF,\n",
    "         lgbm_preds=lgbm_preds,\n",
    "         xgb_preds=xgb_preds,\n",
    "         cat_preds=cat_preds,\n",
    "         et_preds=et_preds,\n",
    "         OOF_std=OOF_std,\n",
    "         OOF_mean=OOF_mean,\n",
    "         OOF_min=OOF_min,\n",
    "         OOF_max=OOF_max,\n",
    "         pred_std=pred_std,\n",
    "         pred_mean=pred_mean,\n",
    "         pred_min=pred_min,\n",
    "         pred_max=pred_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-30 12:35:40,986] A new study created in memory with name: no-name-4dce1a77-d95d-423a-81ee-5d0dd862fb93\n",
      "[I 2024-12-30 12:44:53,988] Trial 0 finished with value: -1.093189955782216 and parameters: {'hidden_layer_sizes': (128,), 'alpha': 0.001902309883123831, 'learning_rate_init': 0.03589685347393951}. Best is trial 0 with value: -1.093189955782216.\n",
      "[I 2024-12-30 12:48:31,307] Trial 1 finished with value: -1.0973197475704348 and parameters: {'hidden_layer_sizes': (64,), 'alpha': 0.0355846550468324, 'learning_rate_init': 0.09652935270287906}. Best is trial 0 with value: -1.093189955782216.\n",
      "[I 2024-12-30 13:03:09,342] Trial 2 finished with value: -1.0595535365866153 and parameters: {'hidden_layer_sizes': (64, 32), 'alpha': 0.002234138975871941, 'learning_rate_init': 0.0001376235781803412}. Best is trial 2 with value: -1.0595535365866153.\n",
      "[I 2024-12-30 13:58:12,612] Trial 3 finished with value: -1.0595774593455944 and parameters: {'hidden_layer_sizes': (64, 32), 'alpha': 0.00046801175151903586, 'learning_rate_init': 0.011263170373780915}. Best is trial 2 with value: -1.0595535365866153.\n",
      "[I 2024-12-30 15:31:59,791] Trial 4 finished with value: -1.0471217891981346 and parameters: {'hidden_layer_sizes': (64, 32), 'alpha': 0.009512335506083183, 'learning_rate_init': 0.00040041326023672764}. Best is trial 4 with value: -1.0471217891981346.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -1.0471217891981346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(64,), (128,), (64, 32)])\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-4, 1e-1)\n",
    "    learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
    "\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=1000,\n",
    "        # early_stopping=True,  #   \n",
    "        # n_iter_no_change=100,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(mlp, stacked_train, y_log_nan, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    return score.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(f\"Best score: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: {'hidden_layer_sizes': (64, 32), 'alpha': 0.009512335506083183, 'learning_rate_init': 0.00040041326023672764}\n",
      "Best score: -1.0471217891981346\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "print(f\"Best alpha: {best_params}\")\n",
    "print(f\"Best score: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_value Ridge     X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_meta = MLPRegressor(\n",
    "#     **best_params, \n",
    "#     max_iter=1000, \n",
    "#     early_stopping=True, \n",
    "#     n_iter_no_change=100,\n",
    "#     random_state=SEED\n",
    "#     )\n",
    "# mlp_meta.fit(stacked_train, y_log_nan)\n",
    "\n",
    "# final_preds = np.expm1(mlp_meta.predict(stacked_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Premium Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200000</td>\n",
       "      <td>1102.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200001</td>\n",
       "      <td>1102.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200002</td>\n",
       "      <td>1102.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1200003</td>\n",
       "      <td>1102.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1200004</td>\n",
       "      <td>1102.545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  Premium Amount\n",
       "0  1200000        1102.545\n",
       "1  1200001        1102.545\n",
       "2  1200002        1102.545\n",
       "3  1200003        1102.545\n",
       "4  1200004        1102.545"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Premium Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200000</td>\n",
       "      <td>785.426348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200001</td>\n",
       "      <td>789.925756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200002</td>\n",
       "      <td>794.167218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1200003</td>\n",
       "      <td>807.570608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1200004</td>\n",
       "      <td>759.075779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  Premium Amount\n",
       "0  1200000      785.426348\n",
       "1  1200001      789.925756\n",
       "2  1200002      794.167218\n",
       "3  1200003      807.570608\n",
       "4  1200004      759.075779"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['Premium Amount'] = preds\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 19.8M/19.8M [00:00<00:00, 46.5MB/s]\n",
      "Successfully submitted to Regression with an Insurance Dataset"
     ]
    }
   ],
   "source": [
    "submission.to_csv('./data/04_03.csv', index=False)\n",
    "!kaggle competitions submit -c playground-series-s4e12 -f \"./data/04_03.csv\" -m \"04_03_stacking_01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Public Score : \n",
    "1. 1.04477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
